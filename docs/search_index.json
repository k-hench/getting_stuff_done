[
["index.html", "Getting Stuff Done 1 Intro", " Getting Stuff Done Kosmas Hench 2019-03-08 1 Intro Bioinformatic analysis can be quite messy at times - complex data wrangling operations, multiple versions of the same analysis (Supervisor: ‘Hey - we should add this bit!’; later: ‘Hey - we should drop that bit…’), parallel working environments (local/ cluster) are all out for your sanity. The following is a presentation of how I (currently) organize my stuff to not get lost. Of course there are probably a zillion of other ways you can organize your workflow and there are some aspects that might be helpful but I did not have the time yet to implement/get to know yet. Therefore this tutorial surely is biased towards my personal preferences/experience and is merely meant as template that you should tweak to you own liking. It will cover the following topics: Basic Setup: Organization of your files &amp; how to get to the command line Bash: The native Linux language (basics) Cluster: How to get to the (GEOMAR) high performance computing cluster Git: version control, connectivity &amp; collaboration R RStudio, RStudio projects &amp; my favorite packages Nextflow: organizing your analysis "],
["basic-setup.html", "2 Basic setup 2.1 Organization 2.2 Tools", " 2 Basic setup 2.1 Organization Generally, I like to keep stuff together if it used within the same project. This means that I recommend to create one project folder for each of your projects. I call this folder the root folder of the projects. This folder will later be equivalent to the git repository and the RStudio projects. Keeping everything needed for the analysis within a single folder has the advantage that you can easily share you work once your project is done. This makes it easy to include your actual analysis within your paper/ report. My personal standard is to (try to) provide all information needed to recreate my studies from raw data to the final figures. This asks for more than a loose collection of programming scripts: Apart from the scripts you ran, people need to know what to run when and where. At least for me, it is usually quite hard even to recall the exact order of my own scripts needed for a complex data analysis (eg. the genotyping process of sequencing data) when I come back half a year later (trying to put together the final version of the methods for a publication). So for someone else it is basically impossible to know how to combine the individual steps of your analysis unless you really make an effort to help them. A first way to make you analysis better understandable is to have a clear structure for you files - having a single project folder is the first step. So, my projects usually look something like this: root_folder ├── analysis_twisst.nf # The nextflow pipeline with the project ├── analysis.Rproj # The RStudio project for the project ├── data # Folder containing the raw input data ├── docs # Folder containing the documentation of the project ├── .git # The housekeeping folder of git ├── .gitignore # List with files that git ignores ├── nextflow.config # nextflow configuration of the project ├── py # Folder containing the python scripts used during analysis ├── R # Folder containing the R scripts used during analysis ├── README.md # Basic readme for the project └── sh # Folder containing the bash scripts used during analysis 2.2 Tools When it comes to the working environment many decisions are ultimately a question of personal taste. Nevertheless, in the following I want to recommend three main pillars of my working environment that I think are essential for bioinformatics. 2.2.1 The command line If you’re doing bioinformatics you will need to use the command line - this is where all the interesting stuff happens. Many programs that are commonly used can only be run from the command line. And if you want to do serious computations using a computer cluster they require that you use the command line. When using the command line, you simply change the way you communicate with your computer: The basic Idea here is to replace your mouse with your keyboard - instead of clicking things you write commands. Yet there are many things that you can do both using the command line or the mouse (eg. creating/ managing folders &amp; files), so the need to write down everything into a “black box” might seem a little tedious at first. Still we use the command line because there are some things that you can only do there. A second huge benefit is that you can protocol everything you do. 2.2.1.1 Linux/ Mac If your OS happens to be Mac or Linux (Ubuntu) you are lucky when it comes to the command line since the command line is a native unix environment. So all you have to do is to look for the Terminal within you preinstalled programs and open it. Yet on Mac you will need to install Xcode to unlock the full potential of the command line. This is quite big and might take some time - sorry… 2.2.1.2 Windows Unfortunately, Windows does not come with a (bash-) Terminal out of the box. So if you are using Windows, you will need to install Cygwin to be able to use all the tools of the command line. Beware that Cygwin will only be able to access a sub directory of you file system - your project folder should be placed within that directory. In such a case (as in most programming related issues) google/duckduckgo is your friend…. 2.2.2 Atom (Text editor/ project manager) To manage your project I recommend using Atom. This is where I keep the track of the entire project, write the pipelines for my analysis, communicate with github - in short this my main working environment. It is cross-platform (Linux/Mac/Windows), integrates git and has a ton of extensions so you can basically puzzle together all the functions you could ever ask for. Yet, I also use gedit (the native text editor) for quick and dirty work and a simple text editor (Mac: textedit, Windows: Notepad) has its value. Sometimes Atom is simply an overkill. 2.2.3 RStudio By now I think R and RStudio are almost synonymous for most users. In my opinion there is no reason not to use Rstudio to develop your R scripts. That said, I view RStudio as a workshop. I use it when I want to work with R interactively - that is for data exploration or to devellop R scripts. My ultimate goal for shareable/reproducible content (eg. scripts for figures in publications) are standalone R scripts. These are executable from the command line and run from start to end without interactive fiddling, eg: Rscript --vanilla script.R or Rscript --vanilla script.R input_file.txt output_figure.pdf So much for now, we’re going to talk more about R later…. 2.2.4 Disclaimer I happen to use Ubuntu on my Laptop and sometimes there are minor differences between commands run under Linux/Mac/Windows. If you encounter weird error this might be the source of the problem. I ran into issues eg. when using awk or sed - we will talk about these later. Also, due to my educational history you will find this tutorial to be quite R-centric. This is reflects my own skill set and many helpful tools especially in the python world are not covered here. "],
["bash.html", "3 Bash 3.1 Commands/Programs (I/II) 3.2 Paths 3.3 Variables 3.4 Scripts 3.5 Commands/Programs (II/II) 3.6 Loops 3.7 Installing new software 3.8 Appendix (sed and awk examples)", " 3 Bash The Bourne-again shell (bash) is usually the programming language that you will use when running the command line. To be efficient you will therefore need some knowledge of this language. I do not intend to rewrite the 1254th tutorial on “How to use bash” since there are allready lots of tutorials online (again - google is your friend here…). Here I will give a small overview and simply list the common patterns/issues/programs and the way I deal with them in bash: 3.1 Commands/Programs (I/II) The first thing you need to know is how to navigate using the command line. The following commands help you with that: pwd is short for “print working directory” and reports the location on your file system where the command line is currently operating (this tells you where you are) ls will print the content (the files &amp; folders) of the current directory (what is around you) cd is short for “change directory” and allows you to change your working directory (move around) echo allows you to print some text as output (it allows you to speak) ./ is the current directory .. is the parent directory (the directory which contains the current directory) With this you can do the most basic navigation. In our assumed project this could look like this: pwd #&gt; /home/khench/root_folder ls #&gt; analysis.Rproj #&gt; analysis_twisst.nf #&gt; data #&gt; docs #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh cd data pwd #&gt; /home/khench/root_folder/data ls #&gt; genotypes.vcf.gz #&gt; table1.txt #&gt; table2.txt cd .. ls #&gt; analysis.Rproj #&gt; analysis_twisst.nf #&gt; data #&gt; docs #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh 3.2 Paths A path is a location on your file system - it is quite similar to a URL in your web browser. It can either point to a file or to folder. We have seen this before: the command pwd prints the path to the current working directory. Generally we need to be aware of two different types - absolute vs. relative paths: An absolute path looks something like this: /home/khench/root_folder (Note the leading slash /home....) This is what the type of path that pwd reports and it will always point to the same location on your file system regardless of the directory you are currently operating in. That is because the leading slash points to the root folder of your file system (not of your project) which is an absolute position. The disadvantage of absolute paths is that things can change: you might move your project folder to a different location (backup on external hard drive) or share with collaborators. In these cases your path will point to a file/location that does not exist - it is basically a broken link. Therefore in some cases the use of relative paths is useful. Relative paths indicate the location of a file/folder relative to your current working directory. We used this for example in the command cd data (short for cd ./data). Here, there is no leading slash - instead the path starts directly with the folder name or with the current directory (./) So to get from the root_folder into the data folder we can use either of the two commands: cd data (relative path) cd /home/khench/root_folder/data (absolute path) 3.3 Variables Variables are containers for something else. You can for example store some text in a variable. Later you can access this “information”. Therefore you have to address the variable name and put it behind a dollar sign: VAR=&quot;some text&quot; echo ${VAR} #&gt; some text (There are two equivalent notations: $VAR and ${VAR}. The notation with curly brackets helps if your variable name is more complex since the start and end of the name is exactly defined.) Variables are often used to store important paths. You could for example store the (absolute) location of your project folder in a Variable (PROJ=&quot;/home/khench/root_folder&quot;) in some sort of configuration script and then use the variable to navigate (cd $PROJ/data). That way you only need to update the location of the project folder in a single place in case it needs to be updated. There are quite a view variables that are already defined on your computer and it is good to be aware of these. Two important ones are $HOME and $PATH. $HOME directs to the home folder of the user (often you can also use ~/ as an equivalent to $HOME/): echo ${HOME} test=&#39;asre&#39; #&gt; /home/khench $PATH often not a directory but a collection of directories separated by a colon: /usr/bin:/usr/local/bin:/home/khench/bin The $PATH is a super important variable - it the register of directories where your computer looks for installed software. Every command that you type into the command line (eg. cd or ls) is a program that is located in one of the folders within your $PATH. You can still run programs that are located elsewhere, but if you do so you need to specify the absolute path of this program (/home/khench/own/scripts/weird_script.py instead of just weird_script.py). We will discuss later how to extend the $PATH in case you want to include a custom software folder if you need to install stuff manually. 3.4 Scripts So far, we have been working interactively on the command line. That is we typed directly into the terminal and observed the direct output. But I claimed before that one of the advantages of the command line is that reproducible and the possibility to protocol the work on the command line. One aspects of this is the ability to store your workflow in scripts. If you use a script to store bash commands the conventional suffix is .sh (eg: script.sh). Additionally it is useful to add a header line that points to the location of bash itself (usually one of the two): #!/bin/bash or #!/usr/bin/env bash A full (admittedly quite silly) script might look like this: #!/usr/bin/env bash cd /home/khench/root_folder ls echo &quot; --------------- &quot; pwd Provided the script is located in our sh folder you can run it like this: bash /home/khench/root_folder/sh/script.sh #&gt; analysis.Rproj #&gt; analysis_twisst.nf #&gt; data #&gt; docs #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh #&gt; --------------- #&gt; /home/khench/root_folder The big benefit of using bash scripts is that you will be able to remember later what you are doing right now. A workflow that you do interactively is basically gone in the sense that you will never be able to remember it exactly. As with the paths there is one script that you should be aware of - your bash start up script. This is a hidden file (.bashrc or .bash_profile)located in your $HOME folder. This script is run every time when you open the terminal and will be important later. 3.5 Commands/Programs (II/II) 3.5.1 Flags One important feature of most command line programs is the usage of flags. These are (optional) parameters that alter they way a program operates an are invoked with - or -- (depending on the program): ls -l #&gt; total 20 #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.Rproj #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis_twisst.nf #&gt; drwxrwxr-x 2 khench khench 4096 Mär 8 10:27 data #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:47 docs #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 nextflow.config #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:48 py #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:48 R #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:48 README.md #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 17:25 sh ls -lth #&gt; total 20K #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 8 10:27 data #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 17:25 sh #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:48 README.md #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:48 R #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:48 py #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 nextflow.config #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:47 docs #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.Rproj #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis_twisst.nf Arguably one of the most important flags for most programs is -help/--help. As you might guess this will print the documentation for most programs. Often this includes an example of the input the program expects, as well as all the options available. 3.5.2 More commands/programs Apart from the most basic commands needed for navigating within the command line, I want to list the commands I personally use most frequently to actually do stuff: 3.5.2.1 Operators # # comment code (stuff here is not executed) &gt; # redirect output into file (overrides existing file) &gt;&gt; # append existing file (creates new file if not yet existent) 1&gt; # redirect stdout to file (1&gt;&gt; append) 2&gt; # redirect stderr to file (2&gt;&gt; append) 2&gt;&amp;1 # redirects stderr to stdout &amp;&gt; # redirect both stdout and stderr to file | # the &#39;pipe&#39;: combine commands * # wildcard/joker: ls *.txt lists all files ending int &#39;.txt&#39; \\ # linebreak: continue a command on a new line (to avoid horribly long comands) It is important to be aware of several channels that are being used within bash: stdin: usually what you type into the terminal stdout: output created by a program stderr: errors created by a program You can use the operators described above to log for example errors and/or output of a program in a log file. 3.5.2.2 Reading Text echo (-e, &quot;\\n&quot;, &quot;\\t&quot; ) # print text to stdout, -e allows special chracters eg newline &amp; tab cat # read a text file and print to stdout zcat # read a gz-compressed text file and print to stdout less (&quot;/&quot;) # read a text interactively (type &quot;/&quot; to search for a pattern) head (-n, -n -1) # print the first (-n) lines of a text file (-1: everything but the last (1) lines) tail (-n , -n +2) # print the last (-n) lines of a text file (2+: everything starting at line (2)) grep (-v, -w, -i, &#39;a\\|b&#39;) # search for line (-v not) containing pattern within text file (a &quot;or&quot; b) wc (-l, -c) # count lines/chracters within text file 3.5.2.3 Text manipulation (table like text files) paste # combine columns sort # sort textfiles based on specific column 3.5.2.4 Text manipulation (table like text files) sed # search and replace awk # power horse: filter rows, combines culumns, complex operations cut (-c, -f) # print selective columns/charaters of each row In my opinion especially sed &amp; awk are extremely powerful commands. I will give a few usage examples at the very end of this section. 3.5.2.5 Text editors There are several actual text editors for the command line: emacs nano vim All have their own fan base. It makes sense to learn how to use at least one of those - basically all will require to learn a handful of key-combinations. 3.5.2.6 Organizing files mkdir # create new directory ln (-s) # create link to file/directory rm (-r) # delete file/directory mv # move/rename file/directory gzip # compress file (file.txt -&gt; file.txt.gz) gunzip # decompress file (file.txt.gz -&gt; file.txt) tar (-zcvf/-zxvf) # (de)compress folder (folder -&gt; folder.tar.gz) 3.5.2.7 Organizing software which (programm_name) # look for the programm called &quot;programm_name&quot; and print its path bash (script.sh) # run &quot;script.sh&quot; within own bash session source (script.sh) # run &quot;script.sh&quot; within current bash session 3.6 Loops Sometimes you need to do one thing multiple times. Or almost the same thing and just modify a single parameter. In this case you can use loops instead of manual copy-and-paste. There are two loops that you should know: for while for k in A B C; do echo $k; done #&gt; A #&gt; B #&gt; C You and use bash commands to create the sequence to loop over by using $(command) (eg: $(ls *.txt)). for k in $(seq -w 9 11); do echo &quot;LG&quot;$k; done #&gt; LG09 #&gt; LG10 #&gt; LG11 You can also loop over the content of a file: cat data/table1.txt #&gt; line1 #&gt; line2 #&gt; line3 Z=1 for k in $(cat data/table1.txt); do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done #&gt; --- 1 ---- #&gt; line1 #&gt; --- 2 ---- #&gt; line2 #&gt; --- 3 ---- #&gt; line3 Yet this might give unexpected results when your file contains whitespaces (looping over a table): cat data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 Z=1 for k in $(cat data/table2.txt); do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done #&gt; --- 1 ---- #&gt; A #&gt; --- 2 ---- #&gt; 1 #&gt; --- 3 ---- #&gt; B #&gt; --- 4 ---- #&gt; 2 #&gt; --- 5 ---- #&gt; C #&gt; --- 6 ---- #&gt; 3 In this case you can switch to while: Z=1 while read k; do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done &lt; data/table2.txt #&gt; --- 1 ---- #&gt; A 1 #&gt; --- 2 ---- #&gt; B 2 #&gt; --- 3 ---- #&gt; C 3 I use this pattern to read in parameters eg. for a function call within the loop: while read k; do P1=$(echo $k | awk &#39;{print $1}&#39;); P2=$(echo $k | awk &#39;{print $2}&#39;); echo &quot;parameter 1: ${P1}, parameter 2: ${P2}&quot; done &lt; data/table2.txt #&gt; parameter 1: A, parameter 2: 1 #&gt; parameter 1: B, parameter 2: 2 #&gt; parameter 1: C, parameter 2: 3 3.7 Installing new software This is hell! Installing new software is what can take up a huge portion of your time &amp; sanity. That is because most new programs that you want to install will not work straight out of the box but depend on 17 other programs, packages and libraries and all of course in specific versions winch contradict each other. These dependencies will then of course not work straight out of the box but depend on … (You get the idea - right?) Ok, I might exaggerate a little here but not much… Therefore, whenever possible try to use package managers. These are programs that try to keep track of your software and - well - to manage the whole dependency hell for you. Depending on your OS, there are different package managers available for you: Mac: homebrew Ubuntu: apt-get (comes with the OS) Universal: conda/bioconda (Conda &amp; especially bioconda are generally pretty useful, but lately they seem a little buggy. That is, lately the take ages to load and their ‘catalog’ might not contain the bleding edge version of the program you’re looking for.) Yet, sometimes the program you need is not available using package managers. In those cases you will have to bite the bullet and install the manually. How to do this exactly varies case by case, but a common theme to compile a program from source is the following: ./configure make make install One example here would be the installation of Stacks: wget http://catchenlab.life.illinois.edu/stacks/source/stacks-2.3d.tar.gz # downloading the software tar xfvz stacks-2.3d.tar.gz # decompressing the folder cd stacks-2.3d # navigate into the folder ./configure make sudo make install You might notice that here we used sudo make install instead of make install. This means that to execute this command we need admin rights - no problem on your laptop, but on the cluster this is a no-go. We’ll talk about this later. The (naive) way I like to picture the installation is by comparing it to building IKEA furniture: ./configure: Your computer reads the IKEA manual and checks that is has all the tools needed to build the piece make: Your computer unpacks the pieces and puts the closet together (it is now standing in your workshop) make install: You put the closet into the bedroom (which is where you will be looking for your cloths) The point with make install is that it puts the compiled software into one of the standard folders within your $PATH so that the computer can find it. But since these folders are quite important the average user is not allowed to modify them - you need admin rights (sudo ...,“become root”) to do this. As mentioned before , this is possible if you own the computer, but on the cluster this will not be the case. The work-around is to create a custom folder that you can acess and to collect the manually installed software there instead. Lets say your software folder is /home/khench/software. This would change your installation procedure to the following: ./configure --prefix=&quot;/home/khench/software&quot; make make install # no &#39;sudo&#39; needed since I &#39;own&#39; the target folder This will place the compiled software into /home/khench/software or /home/khench/software/bin. Our problem is only half solved at this point, since the computer still does not find the software. Assuming the program is called new_program, running which new_program will still return a blank line at this point. Therefore, we need to add /home/khench/software &amp; /home/khench/software/bin to our $PATH. To do this we add the following line to our bash start-up script ($HOME/.bashrc or $HOME/.bash_profile): export PATH=$PATH:/home/khench/software:/home/khench/software/bin We append the existing $PATH with our two custom directories. Note that the start-up script is a start-up script - the changes will come into effect once we restart the terminal but they will not effect the running session. To update the current session we need to run the start-up script manually (source $HOME/.bashrc or source $HOME/.bash_profile). At this point the following should work: which new_program #&gt; /home/khench/software/bin/new_program I usually like to double check that the program will open properly by calling the program help after installing. new_program -help #&gt; Program: new_program (Program for demonstation purposes) #&gt; Version: 0.1.2 #&gt; #&gt; Usage: new_program &lt;options&gt; input_file #&gt; #&gt; Options: -option1 program option 1 #&gt; -option2 program option 1 #&gt; -help display this help text If this does not produce an error but displays the help text, the program should generally work. 3.8 Appendix (sed and awk examples) A large portion of bioinformatics is dealing with plain text files. The programs sed and awk are very powerful for this and can really help you working efficiently. I sure you can do way more using these two commands if you dig into their manuals, but here is how I usually use them: 3.8.1 sed This is my go-to search &amp; replace function. I use it to reformat sample names from genotype files ( -&gt; ), reformat variable within a bash pipeline, transform whitespaces (“” -&gt; “”; &quot; &quot; -&gt; “”) and similar tasks: The basic structure looks like sed 's/pattern/replace/g' &lt;input file&gt;. Here the s/ activates the search &amp; replace mode, the /pattern/ is the old content (search), the /replace/ is the new content (replace) and the /g indicates that you want to replace all occurences (globally) of the patterns within every line of the text. In contrast sed 's/pattern/replace/' would only replace the first occurrence of the pattern within each line. cat data/table1.txt #&gt; line1 #&gt; line2 #&gt; line3 sed &#39;s/line/fancy new content\\t/g&#39; data/table1.txt #&gt; fancy new content 1 #&gt; fancy new content 2 #&gt; fancy new content 3 In cases where you need to replace a slash (“/”) you can use a different delimiter. The following commands are equivalent and I believe there are many more options: s/pattern/replace/g s=pattern=replace=g s#pattern#replace#g You can also replace several patterns in one command (one after ther other) by seperating them with a semicolon (sed 's/pattern1/replace1/g; s/pattern2/replace2/g'). When using sed in a bash pipeline is looks like this: echo -e &quot;sequence1:\\tATGCATAGACATA&quot; | \\ sed &#39;s/^/&gt;/; s/:\\t/\\n/&#39; | gzip &gt; data/test.fa.gz zcat data/test.fa.gz &amp; rm data/test.fa.gz #&gt; gzip: data/test.fa.gz: No such file or directory Generally (also eg. when using grep), there are two important special characters: ^: the start of a line $: the end of a line So, in the example above (sed 's/^/&gt;/') we introduced a “&gt;” at the start of each line (before crating new lines by introducing a linebreak \\n) So far we have replaced patterns in a destructive manner. By this I mean that after using sed the pattern is replaced and thus gone. But sometimes you don’t want to delete the pattern you are looking for but to merely modify it. Of course you could argue that all you need to do is to write eg: echo -e &quot;pattern1234 &amp; pattern3412 &amp; pattern1643&quot; | \\ sed &#39;s/pattern/pattern replace/g&#39; #&gt; pattern replace1234 &amp; pattern replace3412 &amp; pattern replace1643 But this only works when we know exactly what we are llokin for. To be more precise so far we did not really do s/pattern/replace/g but more something like s/name/replace/g. By this I mean that we searched for an exact string (wat I call a name), while a pattern can be more ambiguos by using wildcards and regular expressions (.*,[123] ,[0-9], [0-9]*, [A-Z]*,[a-z]*): We could for example replace all (lowercase) words followed the combination of numbers starting with “1”: echo -e &quot;word1234 &amp; name3412 &amp; string1643 &amp; strinG1643&quot; | \\ sed &#39;s/[a-z][a-z]*1[0-9]*/---/g&#39; #&gt; --- &amp; name3412 &amp; --- &amp; strinG1643 Now in this case, we don’t know the exact string that we are going to replace - we only know the pattern. So if we want to modify but avoid deleting it we need a different method to capture the detected pattern. To do this we fragment the search pattern using \\(pattern\\) or \\(pat\\)\\(tern\\). The patterns declared like s/\\(pattern\\)/ will still be found juust like in s/pattern/, but now we can acess the matched pattern using \\1 (pattern) or \\1 (pat) &amp; \\2 (tern) in the replace section: echo -e &quot;word1234 &amp; name3412 &amp; string1643 &amp; strinG1643&quot; | \\ sed &#39;s/\\([a-z][a-z]*\\)1\\([0-9]*\\)/\\1--&gt;1\\2/g&#39; #&gt; word--&gt;1234 &amp; name3412 &amp; string--&gt;1643 &amp; strinG1643 3.8.2 awk I feel like awk is more like its own programming language than just a unix command - it can be super useful. I usually use it whe “I need to work with columns” within a bash pipeline. This could be eg. add in two columns or add a string to a column based on a condition. I’m afraid this is almost an insult to the programm because I sure you can do waaay cooler things that this - alas, so far I could not get past RTFM. The basic structure of awk looks like: awk &lt;definig variables&gt; &#39;condition {action} condition {action}...&#39; input_file The most simple (and useless) version is to emulate cat: awk &#39;{print}&#39; data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 The first thing to know about awk is how to address the individual columns. By default awk uses any whitespace (&quot; &quot; &amp; &quot;\\t&quot;) as column delimiter. $0 is the whole line, $1 is the first column, $2 is the second … Reading a &quot;\\t&quot; delimited table: awk &#39;{print $2}&#39; data/table2.txt #&gt; 1 #&gt; 2 #&gt; 3 awk &#39;{print $2&quot;-input-&quot;$1}&#39; data/table2.txt #&gt; 1-input-A #&gt; 2-input-B #&gt; 3-input-C Space &quot; &quot; is also delimiter: sed &#39;s/^/new_first_column /&#39; data/table2.txt | \\ awk &#39;{print $2&quot;-input-&quot;$1}&#39; #&gt; A-input-new_first_column #&gt; B-input-new_first_column #&gt; C-input-new_first_column The second thing to know is how to add a condition to a action: awk &#39;$2 &gt;= 2 {print}&#39; data/table2.txt #&gt; B 2 #&gt; C 3 Combining conditions with logical and: awk &#39;$2 &gt;= 2 &amp;&amp; $1 == &quot;C&quot; {print}&#39; data/table2.txt #&gt; C 3 Combining conditions with logical or: awk &#39;$2 &gt;= 2 || $1 == &quot;A&quot; {print}&#39; data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 Different actions for different cases: awk &#39;$2 &lt; 2 {print $1&quot;\\t&quot;$2*-1} $2 &gt;= 2 {print $1&quot;\\t&quot;$2*10}&#39; data/table2.txt #&gt; A -1 #&gt; B 20 #&gt; C 30 awk &#39;$2 &lt; 2 {print $0&quot;\\tFAIL&quot;} $2 &gt;= 2 {print $0&quot;\\tPASS&quot;}&#39; data/table2.txt #&gt; A 1 FAIL #&gt; B 2 PASS #&gt; C 3 PASS In awk, &quot;NR&quot; stands for the row number and &quot;NF&quot; stands for the numbers of fields (columns) within that row: echo -e &quot;1\\n1 2\\n1 2 3\\n1 2 3 4\\n1 2 3&quot; | \\ awk &#39;NR &gt; 2 {print NF}&#39; #&gt; 3 #&gt; 4 #&gt; 3 "],
["cluster.html", "4 Cluster", " 4 Cluster "],
["git.html", "5 Git", " 5 Git "],
["r.html", "6 R", " 6 R "],
["nextflow.html", "7 Nextflow", " 7 Nextflow "],
["other-software.html", "8 Other software", " 8 Other software fastqc multiqc samtools vcftools bedtools "]
]
