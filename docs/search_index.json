[
["index.html", "Getting Stuff Done 1 Intro", " Getting Stuff Done Kosmas Hench 2019-03-13 1 Intro Bioinformatic analysis can be quite messy at times - complex data wrangling operations, multiple versions of the same analysis (Supervisor: ‘Hey - we should add this bit!’; later: ‘Hey - we should drop that bit…’), parallel working environments (local/ cluster) are all out for your sanity. The following is a presentation of how I (currently) organize my stuff to not get lost. Of course there are probably a zillion of other ways you can organize your workflow and there are some aspects that might be helpful but I did not have the time yet to implement/get to know yet. Therefore this tutorial surely is biased towards my personal preferences/experience and is merely meant as template that you should tweak to you own liking. It will cover the following topics: Basic Setup: Organization of your files &amp; how to get to the command line Bash: The native Linux language (basics) Cluster: How to get to the (GEOMAR) high performance computing cluster Git: version control, connectivity &amp; collaboration R RStudio, RStudio projects &amp; my favorite packages Nextflow: organizing your analysis "],
["basic-setup.html", "2 Basic setup 2.1 Organization 2.2 Tools", " 2 Basic setup 2.1 Organization Generally, I like to keep stuff together if it used within the same project. This means that I recommend to create one project folder for each of your projects. I call this folder the root folder of the projects. This folder will later be equivalent to the git repository and the RStudio projects. Keeping everything needed for the analysis within a single folder has the advantage that you can easily share you work once your project is done. This makes it easy to include your actual analysis within your paper/ report. My personal standard is to (try to) provide all information needed to recreate my studies from raw data to the final figures. This asks for more than a loose collection of programming scripts: Apart from the scripts you ran, people need to know what to run when and where. At least for me, it is usually quite hard even to recall the exact order of my own scripts needed for a complex data analysis (eg. the genotyping process of sequencing data) when I come back half a year later (trying to put together the final version of the methods for a publication). So for someone else it is basically impossible to know how to combine the individual steps of your analysis unless you really make an effort to help them. A first way to make you analysis better understandable is to have a clear structure for you files - having a single project folder is the first step. So, my projects usually look something like this: root_folder ├── analysist.nf # The nextflow pipeline with the project ├── analysis.Rproj # The RStudio project for the project ├── data # Folder containing the raw input data ├── docs # Folder containing the documentation of the project ├── .git # The housekeeping folder of git ├── .gitignore # List with files that git ignores ├── nextflow.config # nextflow configuration of the project ├── py # Folder containing the python scripts used during analysis ├── R # Folder containing the R scripts used during analysis ├── README.md # Basic readme for the project └── sh # Folder containing the bash scripts used during analysis 2.2 Tools When it comes to the working environment many decisions are ultimately a question of personal taste. Nevertheless, in the following I want to recommend three main pillars of my working environment that I think are essential for bioinformatics. 2.2.1 The command line If you’re doing bioinformatics you will need to use the command line - this is where all the interesting stuff happens. Many programs that are commonly used can only be run from the command line. And if you want to do serious computations using a computer cluster they require that you use the command line. When using the command line, you simply change the way you communicate with your computer: The basic Idea here is to replace your mouse with your keyboard - instead of clicking things you write commands. Yet there are many things that you can do both using the command line or the mouse (eg. creating/ managing folders &amp; files), so the need to write down everything into a “black box” might seem a little tedious at first. Still we use the command line because there are some things that you can only do there. A second huge benefit is that you can protocol everything you do. 2.2.1.1 Linux/ Mac If your OS happens to be Mac or Linux (Ubuntu) you are lucky when it comes to the command line since the command line is a native unix environment. So all you have to do is to look for the Terminal within you preinstalled programs and open it. Yet on Mac you will need to install Xcode to unlock the full potential of the command line. This is quite big and might take some time - sorry… 2.2.1.2 Windows Unfortunately, Windows does not come with a (bash-) Terminal out of the box. So if you are using Windows, you will need to install Cygwin to be able to use all the tools of the command line. Beware that Cygwin will only be able to access a sub directory of you file system - your project folder should be placed within that directory. In such a case (as in most programming related issues) google/duckduckgo is your friend…. 2.2.2 Atom (Text editor/ project manager) To manage your project I recommend using Atom. This is where I keep the track of the entire project, write the pipelines for my analysis, communicate with github - in short this my main working environment. It is cross-platform (Linux/Mac/Windows), integrates git and has a ton of extensions so you can basically puzzle together all the functions you could ever ask for. Yet, I also use gedit (the native text editor) for quick and dirty work and a simple text editor (Mac: textedit, Windows: Notepad) has its value. Sometimes Atom is simply an overkill. 2.2.3 RStudio By now I think R and RStudio are almost synonymous for most users. In my opinion there is no reason not to use Rstudio to develop your R scripts. That said, I view RStudio as a workshop. I use it when I want to work with R interactively - that is for data exploration or to devellop R scripts. My ultimate goal for shareable/reproducible content (eg. scripts for figures in publications) are standalone R scripts. These are executable from the command line and run from start to end without interactive fiddling, eg: Rscript --vanilla script.R or Rscript --vanilla script.R input_file.txt output_figure.pdf So much for now, we’re going to talk more about R later…. 2.2.4 Disclaimer I happen to use Ubuntu on my Laptop and sometimes there are minor differences between commands run under Linux/Mac/Windows. If you encounter weird error this might be the source of the problem. I ran into issues eg. when using awk or sed - we will talk about these later. Also, due to my educational history you will find this tutorial to be quite R-centric. This is reflects my own skill set and many helpful tools especially in the python world are not covered here. "],
["bash.html", "3 Bash 3.1 Commands/Programs (I/II) 3.2 Paths 3.3 Variables 3.4 Scripts 3.5 Commands/Programs (II/II) 3.6 Loops 3.7 Installing new software 3.8 Appendix (sed and awk examples)", " 3 Bash The Bourne-again shell (bash) is usually the programming language that you will use when running the command line. To be efficient you will therefore need some knowledge of this language. I do not intend to rewrite the 1254th tutorial on “How to use bash” since there are allready lots of tutorials online (again - google is your friend here…). Here I will give a small overview and simply list the common patterns/issues/programs and the way I deal with them in bash: 3.1 Commands/Programs (I/II) The first thing you need to know is how to navigate using the command line. The following commands help you with that: pwd is short for “print working directory” and reports the location on your file system where the command line is currently operating (this tells you where you are) ls will print the content (the files &amp; folders) of the current directory (what is around you) cd is short for “change directory” and allows you to change your working directory (move around) echo allows you to print some text as output (it allows you to speak) ./ is the current directory .. is the parent directory (the directory which contains the current directory) With this you can do the most basic navigation. In our assumed project this could look like this: pwd #&gt; /home/khench/root_folder ls #&gt; analysis.nf #&gt; analysis.Rproj #&gt; data #&gt; docs #&gt; logo.svg #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh cd data pwd #&gt; /home/khench/root_folder/data ls #&gt; genotypes.vcf.gz #&gt; table1.txt #&gt; table2.txt cd .. ls #&gt; analysis.nf #&gt; analysis.Rproj #&gt; data #&gt; docs #&gt; logo.svg #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh One thing that makes your life easier when using the command line is the key on your keyboard. If you don’t know what it does try it. It’s basically an auto-complete for your input. If eg. you are sitting in the root_folder and want to move to root_folder/data all you need to type is root_folder/da&lt;tab&gt; and it will auto-complete to root_folder/data. Typing root_folder/d&lt;tab&gt; will not be enough since this is ambiguous since it might complete to either root_folder/data or root_folder/docs. Of course, in this example we only saved a single key stroke, but in real life can save you a lot of (mis-)typing. 3.2 Paths A path is a location on your file system - it is quite similar to a URL in your web browser. It can either point to a file or to folder. We have seen this before: the command pwd prints the path to the current working directory. Generally we need to be aware of two different types - absolute vs. relative paths: An absolute path looks something like this: /home/khench/root_folder (Note the leading slash /home....) This is what the type of path that pwd reports and it will always point to the same location on your file system regardless of the directory you are currently operating in. That is because the leading slash points to the root folder of your file system (not of your project) which is an absolute position. The disadvantage of absolute paths is that things can change: you might move your project folder to a different location (backup on external hard drive) or share with collaborators. In these cases your path will point to a file/location that does not exist - it is basically a broken link. Therefore in some cases the use of relative paths is useful. Relative paths indicate the location of a file/folder relative to your current working directory. We used this for example in the command cd data (short for cd ./data). Here, there is no leading slash - instead the path starts directly with the folder name or with the current directory (./) So to get from the root_folder into the data folder we can use either of the two commands: cd data (relative path) cd /home/khench/root_folder/data (absolute path) 3.3 Variables Variables are containers for something else. You can for example store some text in a variable. Later you can access this “information”. Therefore you have to address the variable name and put it behind a dollar sign: VAR=&quot;some text&quot; echo ${VAR} #&gt; some text (There are two equivalent notations: $VAR and ${VAR}. The notation with curly brackets helps if your variable name is more complex since the start and end of the name is exactly defined.) Variables are often used to store important paths. You could for example store the (absolute) location of your project folder in a Variable (PROJ=&quot;/home/khench/root_folder&quot;) in some sort of configuration script and then use the variable to navigate (cd $PROJ/data). That way you only need to update the location of the project folder in a single place in case it needs to be updated. There are quite a view variables that are already defined on your computer and it is good to be aware of these. Two important ones are $HOME and $PATH. $HOME directs to the home folder of the user (often you can also use ~/ as an equivalent to $HOME/): echo ${HOME} test=&#39;asre&#39; #&gt; /home/khench $PATH often not a directory but a collection of directories separated by a colon: /usr/bin:/usr/local/bin:/home/khench/bin The $PATH is a super important variable - it the register of directories where your computer looks for installed software. Every command that you type into the command line (eg. cd or ls) is a program that is located in one of the folders within your $PATH. You can still run programs that are located elsewhere, but if you do so you need to specify the absolute path of this program (/home/khench/own/scripts/weird_script.py instead of just weird_script.py). We will discuss later how to extend the $PATH in case you want to include a custom software folder if you need to install stuff manually. 3.4 Scripts So far, we have been working interactively on the command line. That is we typed directly into the terminal and observed the direct output. But I claimed before that one of the advantages of the command line is that reproducible and the possibility to protocol the work on the command line. One aspects of this is the ability to store your workflow in scripts. If you use a script to store bash commands the conventional suffix is .sh (eg: script.sh). Additionally it is useful to add a header line that points to the location of bash itself (usually one of the two): #!/bin/bash or #!/usr/bin/env bash A full (admittedly quite silly) script might look like this: #!/usr/bin/env bash cd /home/khench/root_folder ls echo &quot; --------------- &quot; pwd Provided the script is located in our sh folder you can run it like this: bash /home/khench/root_folder/sh/script.sh #&gt; analysis.nf #&gt; analysis.Rproj #&gt; data #&gt; docs #&gt; logo.svg #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh #&gt; --------------- #&gt; /home/khench/root_folder The big benefit of using bash scripts is that you will be able to remember later what you are doing right now. A workflow that you do interactively is basically gone in the sense that you will never be able to remember it exactly. As with the paths there is one script that you should be aware of - your bash start up script. This is a hidden file (.bashrc or .bash_profile)located in your $HOME folder. This script is run every time when you open the terminal and will be important later. 3.5 Commands/Programs (II/II) 3.5.1 Flags One important feature of most command line programs is the usage of flags. These are (optional) parameters that alter they way a program operates an are invoked with - or -- (depending on the program): ls -l #&gt; total 164 #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.nf #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.Rproj #&gt; drwxrwxr-x 2 khench khench 4096 Mär 13 15:43 data #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:47 docs #&gt; -rw-rw-r-- 1 khench khench 142065 Jul 12 2018 logo.svg #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 nextflow.config #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:48 py #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 15:48 R #&gt; -rw-rw-r-- 1 khench khench 439 Mär 11 17:13 README.md #&gt; drwxrwxr-x 2 khench khench 4096 Mär 7 17:25 sh ls -lth #&gt; total 164K #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 13 15:43 data #&gt; -rw-rw-r-- 1 khench khench 439 Mär 11 17:13 README.md #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 17:25 sh #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:48 R #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:48 py #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 nextflow.config #&gt; drwxrwxr-x 2 khench khench 4,0K Mär 7 15:47 docs #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.Rproj #&gt; -rw-rw-r-- 1 khench khench 0 Mär 7 15:47 analysis.nf #&gt; -rw-rw-r-- 1 khench khench 139K Jul 12 2018 logo.svg Arguably one of the most important flags for most programs is -help/--help. As you might guess this will print the documentation for most programs. Often this includes an example of the input the program expects, as well as all the options available. 3.5.2 More commands/programs Apart from the most basic commands needed for navigating within the command line, I want to list the commands I personally use most frequently to actually do stuff: 3.5.2.1 Operators # # comment code (stuff here is not executed) &gt; # redirect output into file (overrides existing file) &gt;&gt; # append existing file (creates new file if not yet existent) 1&gt; # redirect stdout to file (1&gt;&gt; append) 2&gt; # redirect stderr to file (2&gt;&gt; append) 2&gt;&amp;1 # redirects stderr to stdout &amp;&gt; # redirect both stdout and stderr to file | # the &#39;pipe&#39;: combine commands (THIS ONE IS IMPORTANT) * # wildcard/joker: ls *.txt lists all files ending int &#39;.txt&#39; \\ # linebreak: continue a command on a new line (to avoid horribly long commands) It is important to be aware of several channels that are being used within bash: stdin: usually what you type into the terminal stdout: output created by a program stderr: errors created by a program You can use the operators described above to log for example errors and/or output of a program in a log file. 3.5.2.2 Reading Text echo (-e, &quot;\\n&quot;, &quot;\\t&quot; ) # print text to stdout, -e allows special chracters eg newline &amp; tab cat # read a text file and print to stdout zcat # read a gz-compressed text file and print to stdout less (&quot;/&quot;) # read a text interactively (type &quot;/&quot; to search for a pattern) head (-n, -n -1) # print the first (-n) lines of a text file (-1: everything but the last (1) lines) tail (-n , -n +2) # print the last (-n) lines of a text file (2+: everything starting at line (2)) grep (-v, -w, -i, &#39;a\\|b&#39;) # search for line (-v not) containing pattern within text file (a &quot;or&quot; b) wc (-l, -c) # count lines/chracters within text file 3.5.2.3 Text manipulation (table like text files) paste # combine columns sort # sort textfiles based on specific column 3.5.2.4 Text manipulation (table like text files) sed # search and replace awk # power horse: filter rows, combines culumns, complex operations cut (-c, -f) # print selective columns/charaters of each row In my opinion especially sed &amp; awk are extremely powerful commands. I will give a few usage examples at the very end of this section. 3.5.2.5 Text editors There are several actual text editors for the command line: emacs nano vim All have their own fan base. It makes sense to learn how to use at least one of those - basically all will require to learn a handful of key-combinations. 3.5.2.6 Organizing files mkdir # create new directory ln (-s) # create link to file/directory rm (-r) # delete file/directory mv # move/rename file/directory wget &lt;URL&gt; # download file gzip # compress file (file.txt -&gt; file.txt.gz) gunzip # decompress file (file.txt.gz -&gt; file.txt) tar (-zcvf/-zxvf) # (de)compress folder (folder -&gt; folder.tar.gz) 3.5.2.7 Organizing software which (programm_name) # look for the programm called &quot;programm_name&quot; and print its path bash (script.sh) # run &quot;script.sh&quot; within own bash session source (script.sh) # run &quot;script.sh&quot; within current bash session 3.6 Loops Sometimes you need to do one thing multiple times. Or almost the same thing and just modify a single parameter. In this case you can use loops instead of manual copy-and-paste. There are two loops that you should know: for while for k in A B C; do echo $k; done #&gt; A #&gt; B #&gt; C You and use bash commands to create the sequence to loop over by using $(command) (eg: $(ls *.txt)). for k in $(seq -w 9 11); do echo &quot;LG&quot;$k; done #&gt; LG09 #&gt; LG10 #&gt; LG11 You can also loop over the content of a file: cat data/table1.txt #&gt; line1 #&gt; line2 #&gt; line3 Z=1 for k in $(cat data/table1.txt); do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done #&gt; --- 1 ---- #&gt; line1 #&gt; --- 2 ---- #&gt; line2 #&gt; --- 3 ---- #&gt; line3 Yet this might give unexpected results when your file contains whitespaces (looping over a table): cat data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 Z=1 for k in $(cat data/table2.txt); do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done #&gt; --- 1 ---- #&gt; A #&gt; --- 2 ---- #&gt; 1 #&gt; --- 3 ---- #&gt; B #&gt; --- 4 ---- #&gt; 2 #&gt; --- 5 ---- #&gt; C #&gt; --- 6 ---- #&gt; 3 In this case you can switch to while: Z=1 while read k; do echo &quot;--- $Z ----&quot;; echo $k; Z=$(($Z + 1)); done &lt; data/table2.txt #&gt; --- 1 ---- #&gt; A 1 #&gt; --- 2 ---- #&gt; B 2 #&gt; --- 3 ---- #&gt; C 3 I use this pattern to read in parameters eg. for a function call within the loop: while read k; do P1=$(echo $k | awk &#39;{print $1}&#39;); P2=$(echo $k | awk &#39;{print $2}&#39;); echo &quot;parameter 1: ${P1}, parameter 2: ${P2}&quot; done &lt; data/table2.txt #&gt; parameter 1: A, parameter 2: 1 #&gt; parameter 1: B, parameter 2: 2 #&gt; parameter 1: C, parameter 2: 3 3.7 Installing new software This is hell! Installing new software is what can take up a huge portion of your time &amp; sanity. That is because most new programs that you want to install will not work straight out of the box but depend on 17 other programs, packages and libraries and all of course in specific versions winch contradict each other. These dependencies will then of course not work straight out of the box but depend on … (You get the idea - right?) Ok, I might exaggerate a little here but not much… Therefore, whenever possible try to use package managers. These are programs that try to keep track of your software and - well - to manage the whole dependency hell for you. Depending on your OS, there are different package managers available for you: Mac: homebrew Ubuntu: apt-get (comes with the OS) Universal: conda/bioconda (Conda &amp; especially bioconda are generally pretty useful, but lately they seem a little buggy. That is, lately the take ages to load and their ‘catalog’ might not contain the bleding edge version of the program you’re looking for.) Yet, sometimes the program you need is not available using package managers. In those cases you will have to bite the bullet and install the manually. How to do this exactly varies case by case, but a common theme to compile a program from source is the following: ./configure make make install One example here would be the installation of Stacks: wget http://catchenlab.life.illinois.edu/stacks/source/stacks-2.3d.tar.gz # downloading the software tar xfvz stacks-2.3d.tar.gz # decompressing the folder cd stacks-2.3d # navigate into the folder ./configure make sudo make install You might notice that here we used sudo make install instead of make install. This means that to execute this command we need admin rights - no problem on your laptop, but on the cluster this is a no-go. We’ll talk about this later. The (naive) way I like to picture the installation is by comparing it to building IKEA furniture: ./configure: Your computer reads the IKEA manual and checks that is has all the tools needed to build the piece make: Your computer unpacks the pieces and puts the closet together (it is now standing in your workshop) make install: You put the closet into the bedroom (which is where you will be looking for your cloths) The point with make install is that it puts the compiled software into one of the standard folders within your $PATH so that the computer can find it. But since these folders are quite important the average user is not allowed to modify them - you need admin rights (sudo ...,“become root”) to do this. As mentioned before , this is possible if you own the computer, but on the cluster this will not be the case. The work-around is to create a custom folder that you can acess and to collect the manually installed software there instead. Lets say your software folder is /home/khench/software. This would change your installation procedure to the following: ./configure --prefix=&quot;/home/khench/software&quot; make make install # no &#39;sudo&#39; needed since I &#39;own&#39; the target folder This will place the compiled software into /home/khench/software or /home/khench/software/bin. Our problem is only half solved at this point, since the computer still does not find the software. Assuming the program is called new_program, running which new_program will still return a blank line at this point. Therefore, we need to add /home/khench/software &amp; /home/khench/software/bin to our $PATH. To do this we add the following line to our bash start-up script ($HOME/.bashrc or $HOME/.bash_profile): export PATH=$PATH:/home/khench/software:/home/khench/software/bin We append the existing $PATH with our two custom directories. Note that the start-up script is a start-up script - the changes will come into effect once we restart the terminal but they will not effect the running session. To update the current session we need to run the start-up script manually (source $HOME/.bashrc or source $HOME/.bash_profile). At this point the following should work: which new_program #&gt; /home/khench/software/bin/new_program I usually like to double check that the program will open properly by calling the program help after installing. new_program -help #&gt; Program: new_program (Program for demonstation purposes) #&gt; Version: 0.1.2 #&gt; #&gt; Usage: new_program &lt;options&gt; input_file #&gt; #&gt; Options: -option1 program option 1 #&gt; -option2 program option 1 #&gt; -help display this help text If this does not produce an error but displays the help text, the program should generally work. 3.8 Appendix (sed and awk examples) A large portion of bioinformatics is dealing with plain text files. The programs sed and awk are very powerful for this and can really help you working efficiently. I sure you can do way more using these two commands if you dig into their manuals, but here is how I usually use them: 3.8.1 sed This is my go-to search &amp; replace function. I use it to reformat sample names from genotype files ( -&gt; ), reformat variable within a bash pipeline, transform whitespaces (“” -&gt; “”; &quot; &quot; -&gt; “”) and similar tasks: 3.8.1.1 Basics The basic structure looks like sed 's/pattern/replace/g' &lt;input file&gt;. Here the s/ activates the search &amp; replace mode, the /pattern/ is the old content (search), the /replace/ is the new content (replace) and the /g indicates that you want to replace all occurences (globally) of the patterns within every line of the text. In contrast sed 's/pattern/replace/' would only replace the first occurrence of the pattern within each line. cat data/table1.txt #&gt; line1 #&gt; line2 #&gt; line3 sed &#39;s/line/fancy new content\\t/g&#39; data/table1.txt #&gt; fancy new content 1 #&gt; fancy new content 2 #&gt; fancy new content 3 In cases where you need to replace a slash (“/”) you can use a different delimiter. The following commands are equivalent and I believe there are many more options: s/pattern/replace/g s=pattern=replace=g s#pattern#replace#g You can also replace several patterns in one command (one after the other) by separating them with a semicolon (sed 's/pattern1/replace1/g; s/pattern2/replace2/g'). When using sed in a bash pipeline is looks like this: echo -e &quot;sequence1:\\tATGCATAGACATA&quot; | \\ sed &#39;s/^/&gt;/; s/:\\t/\\n/&#39; | gzip &gt; data/test.fa.gz zcat data/test.fa.gz &amp; rm data/test.fa.gz #&gt; gzip: data/test.fa.gz: No such file or directory Generally (also eg. when using grep), there are two important special characters: ^: the start of a line $: the end of a line So, in the example above (sed 's/^/&gt;/') we introduced a “&gt;” at the start of each line (before crating new lines by introducing a line break \\n) 3.8.1.2 Wildcards So far we have replaced patterns in a destructive manner. By this I mean that after using sed the pattern is replaced and thus gone. But sometimes you don’t want to delete the pattern you are looking for but to merely modify it. Of course you could argue that all you need to do is to write eg: echo -e &quot;pattern1234 &amp; pattern3412 &amp; pattern1643&quot; | \\ sed &#39;s/pattern/pattern replace/g&#39; #&gt; pattern replace1234 &amp; pattern replace3412 &amp; pattern replace1643 But this only works when we know exactly what we are looking for. To be more precise so far we did not really do s/pattern/replace/g but more something like s/name/replace/g. By this I mean that we searched for an exact string (what I call a name), while a pattern can be more ambiguous by using wildcards and regular expressions (.*,[123] ,[0-9], [0-9]*, [A-Z]*,[a-z]*): We could for example replace all (lowercase) words followed the combination of numbers starting with “1”: echo -e &quot;word1234 &amp; name3412 &amp; string1643 &amp; strinG1643&quot; | \\ sed &#39;s/[a-z][a-z]*1[0-9]*/---/g&#39; #&gt; --- &amp; name3412 &amp; --- &amp; strinG1643 3.8.1.3 Modifying matches Now in this case, we don’t know the exact string that we are going to replace - we only know the pattern. So if we want to modify but avoid deleting it we need a different method to capture the detected pattern. To do this we fragment the search pattern using \\(pattern\\) or \\(pat\\)\\(tern\\). The patterns declared like s/\\(pattern\\)/ will still be found just like in s/pattern/, but now we can access the matched pattern using \\1 (pattern) or \\1 (pat) &amp; \\2 (tern) in the replace section: echo -e &quot;word1234 &amp; name3412 &amp; string1643 &amp; strinG1643&quot; | \\ sed &#39;s/\\([a-z][a-z]*\\)1\\([0-9]*\\)/\\1--&gt;1\\2/g&#39; #&gt; word--&gt;1234 &amp; name3412 &amp; string--&gt;1643 &amp; strinG1643 3.8.2 awk I feel like awk is more like its own programming language than just a unix command - it can be super useful. I usually use it when “I need to work with columns” within a bash pipeline. This could be eg. add in two columns or add a string to a column based on a condition. I’m afraid this is almost an insult to the program because I sure you can do waaay cooler things that this - alas, so far I could not get past RTFM. 3.8.2.1 Basics The basic structure of awk looks like: awk &lt;definig variables&gt; &#39;condition {action} condition {action}...&#39; input_file The most simple (and useless) version is to emulate cat: awk &#39;{print}&#39; data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 The first thing to know about awk is how to address the individual columns. By default awk uses any whitespace (&quot; &quot; &amp; &quot;\\t&quot;) as column delimiter. $0 is the whole line, $1 is the first column, $2 is the second … Reading a &quot;\\t&quot; delimited table: awk &#39;{print $2}&#39; data/table2.txt #&gt; 1 #&gt; 2 #&gt; 3 awk &#39;{print $2&quot;-input-&quot;$1}&#39; data/table2.txt #&gt; 1-input-A #&gt; 2-input-B #&gt; 3-input-C Space &quot; &quot; is also delimiter: sed &#39;s/^/new_first_column /&#39; data/table2.txt | \\ awk &#39;{print $2&quot;-input-&quot;$1}&#39; #&gt; A-input-new_first_column #&gt; B-input-new_first_column #&gt; C-input-new_first_column 3.8.2.2 Conditions The second thing to know is how to add a condition to a action: awk &#39;$2 &gt;= 2 {print}&#39; data/table2.txt #&gt; B 2 #&gt; C 3 Combining conditions with logical and: awk &#39;$2 &gt;= 2 &amp;&amp; $1 == &quot;C&quot; {print}&#39; data/table2.txt #&gt; C 3 Combining conditions with logical or: awk &#39;$2 &gt;= 2 || $1 == &quot;A&quot; {print}&#39; data/table2.txt #&gt; A 1 #&gt; B 2 #&gt; C 3 Different actions for different cases: awk &#39;$2 &lt; 2 {print $1&quot;\\t&quot;$2*-1} $2 &gt;= 2 {print $1&quot;\\t&quot;$2*10}&#39; data/table2.txt #&gt; A -1 #&gt; B 20 #&gt; C 30 awk &#39;$2 &lt; 2 {print $0&quot;\\tFAIL&quot;} $2 &gt;= 2 {print $0&quot;\\tPASS&quot;}&#39; data/table2.txt #&gt; A 1 FAIL #&gt; B 2 PASS #&gt; C 3 PASS In awk, &quot;NR&quot; stands for the row number and &quot;NF&quot; stands for the numbers of fields (columns) within that row: echo -e &quot;1\\n1 2\\n1 2 3\\n1 2 3 4\\n1 2 3&quot; | \\ awk &#39;NR &gt; 2 {print NF}&#39; #&gt; 3 #&gt; 4 #&gt; 3 3.8.2.3 Variables One to be aware of is the use of variables within awk. You might have noticed that the columns within awk look like bash variables ( eg. $1). But if you try to use a bash variable within awk this will fail: echo &quot;test&quot; | \\ awk &#39;{print $HOME&quot;--&quot;$1}&#39; #&gt; test--test This is not what we expected - that would have been: echo &quot;$HOME--test&quot; #&gt; /home/khench--test The issue here is the use of different quotation marks (single '' vs. double &quot;&quot;). In short - the awk command needs to be wrapped in single quotes, but within these, bash variables don’t work: echo &#39;$HOME&#39; #&gt; $HOME vs. echo &quot;$HOME&quot; #&gt; /home/khench To get around this we can pass the variable to awk before the use of the single quotes: echo &quot;test&quot; | \\ awk -v h=$HOME &#39;{print h&quot;--&quot;$1}&#39; #&gt; /home/khench--test Basically you can store anything within a awk variable and use it within awk: awk -v x=9 -v y=&quot;bla&quot; &#39;$2 &lt; 2 {print $0&quot;\\t&quot;x} $2 &gt;= 2 {print $0&quot;\\t&quot;y}&#39; data/table2.txt #&gt; A 1 9 #&gt; B 2 bla #&gt; C 3 bla One special awk variable is “OFS”. This is the field delimiter and it can be set like any other variable. The following two examples are equivalent (but one is way easier to read/write). echo &quot;A&quot; | \\ awk &#39;{print $1&quot;\\t&quot;$1&quot;_second_column\\tmore_content\\t&quot;$1&quot;_last_column&quot;}&#39; #&gt; A A_second_column more_content A_last_column echo &quot;A&quot; | \\ awk -v OFS=&quot;\\t&quot; &#39;{print $1,$1&quot;_second_column&quot;,&quot;more_content&quot;,$1&quot;_last_column&quot;}&#39; #&gt; A A_second_column more_content A_last_column 3.8.3 Bash one-liners By combining the programs awk, cat, cut, grep and sed using the pipe | you can build quite concise and specific commands (one-liners) to deal with properly formatted data. Over the years I collected some combinations that I regularly use to deal with different types of bioinformatic data. You can find them together with some more useful bash examples within the oneliners.md file in the source repository of this document. ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0.9000 ✔ purrr 0.3.0 ## ✔ tibble 2.0.1 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.3.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() "],
["cluster.html", "4 Cluster 4.1 Login 4.2 Mounting 4.3 Working", " 4 Cluster This Chapter is very much tailored to the NEC computer cluster of the University Kiel (Germany) operation on the batch system (NQSII). On any other cluster, things are going to be differet - talk to your IT department… At this point we are going to talk about how to work on the computer cluster (of the University Kiel). Usually, we want to work there because: We have to deal with LOTS of data and this would kill our laptop or We have to run a job for VERY LONG (and we need the laptop for a presentation that’s due tomorrow…) or We have MANY jobs and running the on after the other would take ages So the cluster is capable of doing many work heavy jobs for a long time. But to use it we need: an account on the cluster (+ password) - lets assume it is smomw000 for now ssh should already be installed (check which ssh in the command line) an internet connection (and access to either the Uni or the GEOMAR network) (sshfs for mounting the cluster) So, I recommend to check at this point that you check if you have sshfs installed - I guess not. If which sshfs returns a path (eg: /usr/bin/ssh) your all set, if it returns a blank line you need to install the program: Ubuntu: sudo apt-get install sshfs Mac: You’ll need to download and install two packages (osxfuse and the current version of sshfs-x.x.x.pkg) Windows: Honestly, I don’t know. But these instuctions look like they should work. Good luck… Now if you double check which sshfs should return a path. 4.1 Login As mentioned before to use the cluster you need the command line - all the work on the cluster is going to happen there. The log in command looks like this: # ssh &lt;username&gt;@&lt;hostadress&gt; ssh smomw000@nesh-fe.rz.uni-kiel.de When you log in for the first time, you will be asked to confirm that this is a secure connection. Then you will need to type in you password and that’s basically it: Last login: Fri Mar 8 20:54:44 2019 from XXX.XXX.XXX.XXX ***************************************************************************************** * Welcome to the hybrid NEC HPC-system at RZ CAU Kiel * * * * Documentation: * * SX-ACE vector system: https://www.rz.uni-kiel.de/de/angebote/hiperf/nec-sx-ace * * Linux-Cluster: https://www.rz.uni-kiel.de/de/angebote/hiperf/nec-linux-cluster * * * * Support: mailto:hpcsupport@rz.uni-kiel.de * ***************************************************************************************** * Your current quota on $WORK=/sfs/fs2/work-geomar7/smomw000: * * [used] | [soft] | [hard] | [grace period] * * disk space: x.xxT | x.xxT | x.xxT | none [-1] * * inodes: xxxxx | xxxxx | xxxxxx | none [-1] * ***************************************************************************************** $ | (As a convention I will use black code blocks when talking about the cluster. The code blocks you are used to will continue to refer to the command line on your laptop.) Now you are on the cluster and you can work just the same as on the command line on your local machine. If you want to leave the cluster, all you need to do is type exit. At this point it is probably a good idea to look at the documentation of the cluster by the HPC-Support team if you have not done so already. Most of what I will cover below is actually described there in more depth and up to date (and by people who actually know the system way better). 4.1.1 File system You should be aware that on the cluster you have access to several directories: $HOME: /sfs/fs6/home-geomar/smomw000 $WORK: /sfs/fs2/work-geomar7/smomw000 $TAPE_CACHE: /nfs/tape_cache/smomw000 The different directories differ in way they are backuped and by the amount of space you can use. They should be used for different tasks: In the $HOME are mostly things that you need to configure your account. On example is the start up script for your cluster account (.bashrc). You might collect configuration content for other programs there as well (eg. R,py or miniconda2). It should not be used to store large amounts of data or to actually do work there. The $WORK directory is where you will spend your time. Here you can store your data and do the heavy lifting run your projects. It is also the only directory where you should run batch jobs. When you are done with a project and don’t need to access your file regularly anymore you can store them on $TAPE_CACHE. This basically the archive. You should prepare your data into neat packages (don’t just dump you messy folders with a huge collection of loose scripts here - pack the collection into a single tar file first: tar -zcvf messy_folder.tar.gz messy_folder). Pleas read the recommendations within the cluster documentation. 4.2 Mounting One of the first things you might ask yourself is how to get you data onto the cluster. For this purpose we installed shhfs at the beginning of this chapter. I like to think of the usage of shhfs like plugging in a flash drive. But before we can use it, we need create an empty folder: mkdir ${HOME}/mnt # Remember: This is on your local machine. This out virtual USB slot - after mounting the cluster the files of the cluster are going to show up here on your laptop. We also need to think about which directory of the cluster we want to mount. By default the $HOME directory of the user will be mounted. But as mentioned above, that’s not were we are supposed to dump our data. Also, once we have mounted a directory we will not be able to go up in the path. That means we will not be able to access /sfs/fs6/home-geomar/ when mounting /sfs/fs6/home-geomar/smomw000. Because of this, we will need to mount the $WORK directory directly: # sshfs &lt;username&gt;@&lt;hostadress&gt;:&lt;/path/on/cluster&gt; &lt;local/path&gt; sshfs smomw000@nesh-fe.rz.uni-kiel.de:/sfs/fs2/work-geomar7/smomw000 $HOME/mnt Now, the content of your $WORK directory should show up in your finder/windows explorer. You can now copy data either by copy &amp; paste in your finder, or using cp large_data_file.vcf.gz $HOME/mnt. To unmount the you can try to eject it (similar to a Flash drive) or type fusermount -u ~/mnt. 4.3 Working Working on the cluster is generally not different to working ton the command line locally. 4.3.1 Software The one exception to this is when you need new software that is not installed. That is because you do not have admin rights on the cluster - this makes installing new software harder (s. 3.7. Installing new software). Luckily, there is more stuff installed than you might realize at first - it is just not activated yet. To see whats available run module avail: ------------------------------ /sfs/fs5/sw/modules/sxcrosscompiling ------------------------------ MathKeisan/4.0.3(default) fview sxf03/rev061(default) crosscompiler mpisx/10.2.4(default) sxf90/rev534(default) crosscompiler.nec netcdf_4.1.1_sx crosskit/r211(default) sxc++/rev112(default) -------------------------------- /sfs/fs5/sw/modules/x86compilers -------------------------------- gcc5.3.0 intel16.0.4 intelmpi16.0.4 java1.8.0 llvm6.0.0 gcc7.2.0 intel17.0.4 intelmpi17.0.4 llvm4.0.1 -------------------------------- /sfs/fs5/sw/modules/x86libraries -------------------------------- boost1.65.0 hdf5-1.8.19intel pcre2-10.21 curl7.55.1 hdf5parallel-1.8.19 pcre8.41 eigen3.3.4 hdf5parallelintel-1.8.19 pnetcdf1.8.1 fftw3.3.6 jags4.3.0 pnetcdf1.8.1intel fftw3.3.6intel lapack3.8.0 proj4.9.3 gdal2.2.3 ncurses6.0 readline7.0 geos3.6.2 netcdf4.4.1 szip2.1.1 glib2.52.3 netcdf4.4.1intel udunits2.2.25 gsl2.4 netcdf4.4.1paraintel xz5.2.3 hdf5-1.8.19 openssl1.0.2 zlib1.2.11 -------------------------------- /sfs/fs5/sw/modules/x86software --------------------------------- R3.4.1 espresso5.4.0 matlab2015a_geomar petsc3.6.1intel-debug R3.5.1 espresso6.2.1 matlab2017a petsc3.7.6 R3.5.2 espresso6.2plumed matlab2017a_geomar petsc3.7.6-debug abaqus2018 fastqpair12.2017 matlab2018b petsc3.7.6intel adf2017.110 ferret6.72 matlab2018b_geomar petsc3.7.6intel-debug adf2017.110intel ferret6.82 megahit1.1.3 plumed2.4.0 allpathslg52488 ferret7.2 metawrap1.0.2 plumed2.4.0intel amos3.1.0 ferret7.4test molpro2015 pyferret7.4.3 beat1.0 fomosto-qssp2017 mothur1.39.5 python2.7.13 blender2.79 g09d01 mothur1.40.0 python3.6.2 blender2.79a g16a03 mummer3.23 salmon0.12.0 blender2.79b glpk4.61 nciplot3.0 samtools1.5 bowtie2-2.3.3 gmt5.4.2 ncl6.4.0 spades3.12.0 bwa0.7.17 gnuplot5.0.7 nco4.6.8 specfem3d3.0 cactus7.2018 grace5.1.9 ncview1.2.7 specfem3d3.0mpi cdo1.9.0 grib_api1.23.1 octave4.2.1 star2.6.0a comsol5.3a-tetra hail0.2 octopus7.1 transrate1.0.3 comsol5.4-tetra interproscan5.30-69.0 openmolcas4.2017 turbomole7.2 comsol5.4s-tetra jasper2.0.14 openmolcas4.2017serial turbomole7.2mpi cp2k5.1 lammps17 perl5.26.0 turbomole7.2smp cplex matlab2011b_geomar perl5.26.0threads cufflinks2.2.1 matlab2015a petsc3.6.1intel ---------------------------------- /sfs/fs5/sw/modules/x86tools ---------------------------------- bzip2-1.0.6 cmake3.9.1 imake1.0.7 mc4.8.19 parallel use.own cmake3.12.1 git2.14.1 libtool2.4.6 miniconda3 sensors3.4.0 You can activate these modules by running eg. module load R3.5.2 (and later deactivate it with module unload R3.5.2). If you look at the very end of the list you will find miniconda3. This is quite a relief because this means you should be able to use conda as package manager to install most of the software you need (especially when coupled with bioconda). (Unfortunately conda apears to be suuuper slow lately. This is annoying but is still works. And I think in most cases it is still way faster than installing all dependencies of your software by hand.) If the software is not part of the modules and if you can’t find it using conda you will have to install it manually. Sorry. 4.3.2 Batch scripts I like to think of the cluster as a kind of hotel - the cluster is a collection of many computers like a hotel is a collection of many rooms. Yet when you enter the hotel, you don’t walk into a room directly but you enter the lobby. On the cluster it is the same: When you log in using ssh you are on the login node (the “front end”). Here you are not alone, but all users of the cluster share it simultaneously. It is totally fine to hang out in the lobby as long as don’t block it with 15 busloads of luggage. This is supposed to mean that, while you can organize you files on the login node and also to try small paths, as soon as you start to work seriously you should use batch jobs (rent one/several rooms). 4.3.3 Preparation Running a batch job is easy. Instead of running commands interactively, you write them into a script and add a small header. This has the additional benefit that later you will remember exactly what you have done. So instead of this: cd $WORK mkdir test_folder echo &quot;test&quot; &gt; test_folder/test.txt We write this script (test.sh): #!/bin/bash #PBS -b 1 # number of threads #PBS -l cpunum_job=1 # number of nodes #PBS -l elapstim_req=00:00:30 # requested runtime (hh:mm:ss) #PBS -l memsz_job=1gb # requested memory #PBS -N testjob # job name #PBS -o stdstderr.out # file for standard-output #PBS -j o # join stdout/stderr #PBS -q clexpress # requested batch class cd $WORK mkdir test_folder echo &quot;test&quot; &gt; test_folder/test.txt You can write this script either using one of the command line text editors (emacs/nano/vim) or you can prepare it locally (eg. within atom) and copy it onto the mounted cluster. Once the script is located on the cluster (eg. on $WORK) you can submit the job: cd $WORK qsub test.sh Depending on the resources needed for the job you submit, you will choose a different batch class: Batch class max. runtime cores per node max. memory max. nodes clexpress 2 hours 32 192 GB 2 clmedium 48 hours 32 192 GB 120 cllong 100 hours 32 192 GB 50 clbigmem 200 hours 32 384 GB 8 clfo2 200 hours 24 128 GB 18 feque 1 hour 32 750 GB 1 4.3.4 Monitor batch jobs After submitting a batch job you might be interested in its current status. You can get a summary of all your currently queuing and running jobs using qstat: RequestID ReqName UserName Queue Pri STT S Memory CPU Elapse R H M Jobs --------------- -------- -------- -------- ---- --- - -------- -------- -------- - - - ---- 000000.ace-ssio test smomw000 clexpress 0 RUN - 00.13G 122.70 1 Y Y Y 1 If you want to see what is going on on the entire cluster you can run qstatall to see all currently queuing and running jobs. I you want to get a peek into the latest output within the job you ca use: qcat -e &lt;jobid&gt;: the latest error messages (the stderr output) qcat -o &lt;jobid&gt;: the latest output messages (the stdout output) qcat &lt;jobid&gt;: the original script that was submitted If you want to see more lines you can use the flag -n. The &lt;jobid&gt; is the number before “.ace-ssio”. An example would be qcat -n 25 -e 000000. If you are impatient (like me) you will find yourself retyping qcat over and over. (If you need to repeat a command use the &lt;arrow up&gt; key on your keyboard!) This can be tiresome. It is way easier to use the command watch. The following line will re-execute qcat every 3 seconds until you interrupt it by pressing &lt;ctrl&gt;&lt;c&gt;: watch -n 3 &quot;qstat -n 25 -e 000000&quot; Once your job is done you will find the file stdstderr.out within the directory where you submitted the original script (where you ran qsub script.sh). "],
["git.html", "5 Git 5.1 Get started 5.2 gitignore 5.3 Conecting to github 5.4 Using git with Atom", " 5 Git This chapter should actually simply read “use version control”. Git just happens to be the most commonly used software to implement version control. Again, I do not intend to write another redundant git tutorial but simply want to show how I currently use git. (Honestly, I’m still learning this myself and people who actually know this stuff might disagree with my approach…) If you want to get a proper git introduction, please read the manual and/or do an actual tutorial. So, how do I integrate git into my work? In the introduction I recommended to keep your whole project within a single folder. We will now turn this folder into a git repository. This means, we’ll use git to keep track of the changes that happen within this project folder. To do this, we will take snapshots of our project every time we feel like we made some progress. Later, we will be able to come back to any time point of which we have a snapshot (it’s a little like saving a computer game…). 5.1 Get started We are going back to our example project folder (~/root_folder) of the chapter 2. All of the git commands have the structure git [command] &lt;options&gt;. To turn the folder into a git repository we run: cd ~/root_folder git init #&gt; Initialized empty Git repository in /home/khench/root_folder/.git/ No, we can check the current status of the repository: git status #&gt; On branch master #&gt; #&gt; Initial commit #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; .gitignore #&gt; README.md #&gt; analysis.Rproj #&gt; analysis.nf #&gt; data/ #&gt; nextflow.config #&gt; sh/ You’ll see that git is aware of all the files (empty folders are omitted) in the folder. But at this stage the filed are not tracked yet. 5.2 gitignore At this point it makes sense to talk about why we need the file .gitignore. As I mentioned before, the huge benefit of git is that it will enable us later to come back to previous versions of our project since we save snapshots throughout the development of the project. Of course this means that all these versions (or at least the changes within the files) need to be stored at some place. So basically we have to deal with a trade-of where we have to choose between tracking as much as possible keeping our repository at a manageable size. Luckily the most important files to keep track of (in my opinion) are also the smallest, while the largest files might actually be omitted without any issue. That is because your actual work - the scripts that you develop for your analysis are small text files. In contrast to this, you raw data might be huge - but it is static and does not necessarily need to be tracked. You will very likely deposit this data anyways at an external data base like eg. dryad, SRA or ENA. And after all, the whole point of our efforts is to provide reproducible scripts to get to our final results starting with the raw data. I therefore recommend to exclude the data folder from the list of files that git keeps track of. We still want to keep them within the project folder because things get messy when you address directories outside the repository. Other people would not be able to recreate your file structure anymore. Because of this we simply add the data folder to the .gitignore file (using a text editor). This is basically a blacklist of files &amp; directories that “git ignores”. I want to do this at the very beginning to not have any traces of the huge files within the history of the repository. .gitignore now looks like this: cat .gitignore #&gt; data Now we first add the .gitignore to the tracked files and only add the rest later. We need to have the .gitignore in effect first - otherwise the data folder will not be omitted: git add .gitignore git status #&gt; On branch master #&gt; #&gt; Initial commit #&gt; #&gt; Changes to be committed: #&gt; (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) #&gt; #&gt; new file: .gitignore #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; README.md #&gt; analysis.Rproj #&gt; analysis.nf #&gt; nextflow.config #&gt; sh/ So, now the changes in .gitignore are staged for a commit - the file is ready for a snapshot. To save this snapshot we run: git commit -m &quot;init gitignore&quot; #&gt; [master (root-commit) 575e91f] init gitignore #&gt; 1 file changed, 1 insertion(+) #&gt; create mode 100644 .gitignore git status #&gt; On branch master #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; README.md #&gt; analysis.Rproj #&gt; analysis.nf #&gt; nextflow.config #&gt; sh/ #&gt; #&gt; nothing added to commit but untracked files present (use &quot;git add&quot; to track) Now we see two things: .gitignore does not show up in the git status report. There are no changes in this file since the last commit. The data folder does not show up in the git status report. Our .gitignore appears to be in effect. So, at this point we can add and commit the rest of the files: git add . git status #&gt; On branch master #&gt; Changes to be committed: #&gt; (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) #&gt; #&gt; new file: README.md #&gt; new file: analysis.Rproj #&gt; new file: analysis.nf #&gt; new file: nextflow.config #&gt; new file: sh/script.sh git commit -m &quot;init repo&quot; #&gt; [master a9f87c1] init repo #&gt; 5 files changed, 6 insertions(+) #&gt; create mode 100644 README.md #&gt; create mode 100644 analysis.Rproj #&gt; create mode 100644 analysis.nf #&gt; create mode 100644 nextflow.config #&gt; create mode 100644 sh/script.sh git status #&gt; On branch master #&gt; nothing to commit, working directory clean 5.3 Conecting to github So far everything is good and well - we do have version control implemented for our project and know how to commit changes after we have developed the project further. Yet, ultimately we want to share the code with other people (or at least make it available to the public.) This means we want to provide the repository to the public online and for this we need a platform. Chances are you have heard of github even if you have not heard of git itself. That is because it is likely the larges platform where many programs (in their form as git repositories) are being shared by their developers. There are other such platforms such as bitbucket or gitlab (which Geomar is using internally). Yet here I will stick to github since this is also what I am using personally (and where this page is hosted). To use github you have to create an account first. The basic (public) account is for free, but if you want to use github for super secret stuff, you will need to get a payed account (which is about 85$ / year I think). 5.3.1 Move the repository to github Now, how do we get the the new repository onto github? First, we log onto github and create an empty repository there by clicking the + in the upper right corner: We choose a name, a small description and toggle public/private (here for an ongoing project I would likely choose ‘private’) and click (Create reopsitory). Then we switch back to the command line: Within the root_folder we now create a connection to the new github repository which we set to be the origin of our local project: git remote add origin https://github.com/k-hench/demo_root_folder.git git push -u origin master #&gt; Counting objects: 8, done. #&gt; Delta compression using up to 4 threads. #&gt; Compressing objects: 100% (4/4), done. #&gt; Writing objects: 100% (8/8), 652 bytes | 0 bytes/s, done. #&gt; Total 8 (delta 0), reused 0 (delta 0) #&gt; To https://github.com/k-hench/demo_root_folder.git #&gt; * [new branch] master -&gt; master #&gt; Branch master set up to track remote branch master from origin. Now we can see the content of our root_folder showing up on our github account: By clicking the “commits” tab: , we can see the history of our repository. These are all the snapshots we took so far, so we find the changes to the .gitignore and the subsequent import of the folder content: 5.3.2 Get the repository from github At this point we can import the repository to other locations from github. This could be to your collaborators computer, to a reviewer of your paper or to the cluster. In my opinion, the beauty of using github it that we can use it for different purposes depending on the stage of our project: Moving content (new scripts) from your machine to the cluster (devellopment phase) Backup &amp; documentation of the project (constatntly from beginning to end) Publication of the workflow (after/at publication of the study) Actually that’s probably even the more lame aspects of using git, for as far as I understand the actual benefits start to kick in once you work collaboratively on a single project with other people. Importing the repository to the cluster is easy now. We simply copy the repository URL from github: Then we log in to the cluster and clone the repository. This will download it to the current directory on the cluster: cd $WORK git clone https://github.com/k-hench/demo_root_folder.git #&gt; Cloning into &#39;demo_root_folder&#39;... #&gt; remote: Enumerating objects: 8, done. #&gt; remote: Counting objects: 100% (8/8), done. #&gt; remote: Compressing objects: 100% (4/4), done. #&gt; remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0 #&gt; Unpacking objects: 100% (8/8), done. cd demo_root_folder/ ls -1a #&gt; . #&gt; .. #&gt; .git #&gt; .gitignore #&gt; README.md #&gt; analysis.Rproj #&gt; analysis.nf #&gt; nextflow.config #&gt; sh You might notice that not all the files &amp; folders made it to the cluster though: tree #&gt; . #&gt; |-- analysis.Rproj #&gt; |-- analysis.nf #&gt; |-- data #&gt; | |-- genotypes.vcf.gz #&gt; | |-- table1.txt #&gt; | `-- table2.txt #&gt; |-- docs #&gt; |-- nextflow.config #&gt; |-- py #&gt; |-- R #&gt; |-- README.md #&gt; `-- sh #&gt; `-- script.sh #&gt; #&gt; 5 directories, 8 files tree #&gt; . #&gt; |-- README.md #&gt; |-- analysis.Rproj #&gt; |-- analysis.nf #&gt; |-- nextflow.config #&gt; `-- sh #&gt; `-- script.sh #&gt; #&gt; 1 directory, 5 files So of course the files that are listed on the .gitignore file, but also empty folders are omitted and not exported to github. 5.3.3 Update the repository Now, lets see how to update the content. I usually write the code of my analysis on my laptop and then use github to transfer it to the cluster. To emulate this, lets put some information into the README.md file. By default, this file serves as welcome message on the github project page. Therefore I’ll open the file in a text editor and copy this text: Hello world, This is a showcase repository for my bioinformatics crash course. For more information please go to my repository *getting_stuff_done* or look [here](https://k-hench.github.io/getting\\_stuff\\_done/git.html\\#conecting-to-github). After saving the changes, if we check the repository status in the command line, we’ll get: git status #&gt; On branch master #&gt; Your branch is up-to-date with &#39;origin/master&#39;. #&gt; Changes not staged for commit: #&gt; (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) #&gt; (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) #&gt; #&gt; modified: README.md #&gt; #&gt; no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) So git is aware that something has changed. Like before, we’ll use git add . and git commit -m &quot;update README.md&quot; to save the new changes: git add . git commit -m &quot;update README.md&quot; #&gt; [master c4d6df5] update README.md #&gt; 1 file changed, 6 insertions(+) git status #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 1 commit. #&gt; (use &quot;git push&quot; to publish your local commits) #&gt; nothing to commit, working directory clean Getting the updates to the cluster is a two-step process: git push the changes from the laptop to github git pull the update from github to the cluster git push origin master #&gt; Counting objects: 3, done. #&gt; Delta compression using up to 4 threads. #&gt; Compressing objects: 100% (3/3), done. #&gt; Writing objects: 100% (3/3), 429 bytes | 0 bytes/s, done. #&gt; Total 3 (delta 1), reused 0 (delta 0) #&gt; remote: Resolving deltas: 100% (1/1), completed with 1 local object. #&gt; To https://github.com/k-hench/demo_root_folder.git #&gt; a9f87c1..c4d6df5 master -&gt; master git pull origin master #&gt; remote: Enumerating objects: 5, done. #&gt; remote: Counting objects: 100% (5/5), done. #&gt; remote: Compressing objects: 100% (2/2), done. #&gt; remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0 #&gt; Unpacking objects: 100% (3/3), done. #&gt; From https://github.com/k-hench/demo_root_folder #&gt; * branch master -&gt; FETCH_HEAD #&gt; a9f87c1..c4d6df5 master -&gt; origin/master #&gt; Updating a9f87c1..c4d6df5 #&gt; Fast-forward #&gt; README.md | 6 ++++++ #&gt; 1 file changed, 6 insertions(+) Also, now our README.md is visible on the github project page (and we can see the new commit): At this point we have the basics to develop our project locally, synchronizing the scripts with the cluster (where the heavy duty jobs will be run) and also keep track of the development of our project (Using version control instead of horrible file names). 5.4 Using git with Atom Writing all those git add . &amp; git commit commands might look tedious and impractical. Luckily, this pain is greatly reduced when using Atom or a similar text editor as project manager. When opening a folder with Atom I usually see three panels (from left to right): The file tree that gives on overview over the content of the project The editor area in the middle The git tab showing the current git status If your Atom opens with a blank file instead of a project folder you can open it manually: You can also toggle the git tab under the menu item View: Now we make some changes to README.md and import the hamlet logo into the project. In the screenshot below I: saved the file changes imported the logo staged the addition of the logo (equivalent to git add logo.svg) added the committing comment (equivalent to git commit -m &quot;add hamlet logo.&quot;) You can also easily push changes to github after dong a commit (the fetch at the button turns into push). There are a ton of extensions for Atom out there, so you can custoomize it to your needs. One example is the mini map of the code that you see to right inside the editor tab. "],
["r.html", "6 R 6.1 Read 6.2 Know 6.3 Run", " 6 R I could probably write 30 pages about my opinion on different R topics but I’ll spare you the pain of going over this here. All I want to do at this point, is to point out some of the excellent texts &amp; tutorials that really push my own coding in R. Then I need to mention 1/4 packages that anyone writing in R these days should be aware of. If you know them and choose not to use them - well that’s your issue. But if you have not looked at them so far - do it now! (Otherwise you’ll have serious issues when trying to follow any of my R code…) The last issue I want to highlight is the use of standalone R scripts that are meant for used from the command line/ within a pipe. For me this is my attempt to create reproducible analysis that removes as much subjectivity as possible from the last part of the from raw data to figures journey. 6.1 Read Anything written by Hadley Wickham will improve your R. He has written three great books (which are all publicly available for free) R for Data Science R packages Advanced R If just start to get going with ggplot2: the R Graphics cook book a tutorial by the Harvad University this page by Zev Ross Also, this platform of ggplot extensions is definitively worth a look. 6.2 Know You should know the tidyverse. It not one, but actually a collection of packages which make working in R just so much more pleasant, streamlined readable, fun… The biggest impact so far on my own coding came from four packages that are part of the tidyverse. I list them below together with those functions from these packages which really made a difference. magrittr: the pipe operator (%&gt;%) is a game changer for R ggplot2: simply the best way of data visualization I have used so far dplyr: does for tables what ggplot2 does for plots (filter(), select(), mutate(), group_by(), summarise(),left_join(), tutorial) purrr: this package changed the way I thought about code. Since I saw the beauty of map()/map2()/pmap() I pack most of my code into neat small functions. This saves soooooooo much typing/sources of error/confusion. Of course there is many more great packages out there on CRAN, github or bioconductor. But the last package that I’m going to mention here is devtools. With this package you can easily install R packages from all different kinds of sources. 6.3 Run Rstudio is a great environment to develop R code. I spend days using it the idea of running plain R seems almost archaic to me. So during most of the time I will use R code throughout a project. But when it comes to the final scripts that are going to make it to the publication, I want to drop as much interactivity as possible for the sake of reproducibility. I already keep this in mind during the process of my scripts. This means that every script should run from start to end without interactive intervention. No manual loading of data, interaction with other scripts basically only using source() (outsourcing definition of needed custom functions might actually be a good idea). So, after polishing my R scripts to their final version they should be able to be run from the command line: Rscript --vanilla script.R # or Rscript --vanilla script.R input_file.tsv input_value external_script.R For this I provide the scripts that are meant for execution with a small header &amp; config section (not the ones which I source()). Here is an example for the script that takes input from the command line. #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla script.R input_file.tsv input_value external_script.R # =============================================================== # This script provides an example of an executable R script. # It takes thee inputs and parses them into R variables # --------------------------------------------------------------- # The produced output contains: # - nothing, it&#39;s just a demo # =============================================================== # args contails everything thats typed into the command line - # Our input starts with the 7th element. # Careful: if you need more input fields, you also need to adjust the subsetting args = commandArgs(trailingOnly = FALSE) args = args[7:9] print(args) # Quick copy &amp; paste section for devellopment phase # args &lt;- c(&#39;input_file.tsv&#39;, &#39;input_value&#39;, &#39;external_script.R&#39;) # ------------------------------------ # Config: # Set seed if your script uses any element of randomnes (sample(), rnorm(),...) set.seed(42) input_file &lt;- as.character(args[1]) input_value &lt;- as.numeric(args[2]) config_script &lt;- as.character(args[3]) # ------------------------------------ # Here, the content of the original script starts library(tidyverse) source(config_script) data &lt;- read_tsv(input_file) # The code continues.... "],
["nextflow.html", "7 Nextflow 7.1 Basics 7.2 A quick example 7.3 Managing the workflow on the cluster 7.4 The config script 7.5 Quick recap", " 7 Nextflow This is awesome! Nextflow provides a neat way to develop and structure the entire workflow of you analysis in a single script. It makes a huge difference for one aspect I mentioned in Chapter 2: “Apart from the scripts you ran, people need to know what to run when and where.” “At least for me, it is usually quite hard even to recall the exact order of my own scripts needed for a complex data analysis […] when I come back half a year later[…].” Apart from this using Nextflow can save you a lot of mindless typing when doing different versions of a similar analysis. The drawback is that you need to get a basic Idea of yet another language (Nextflow is based on groovy). But the Nextflow documentation is pretty good. Reading through the manual took me quite some time but the time I saved since then more than compensated pretty quickly. Also, if you have questions beyond what is covered in the manual, the help provided on gitter is amazing. So far any question I asked there was solved within about 30 min! (Of course it important not to spam the people there with unnecessary questions - so first: RTFM) 7.1 Basics The basic Idea of Nextflow is that it will connect different task that you do within bash (or any other scripting language) in an organized but dynamic manner. It then manages your analysis for you (waiting for job A to finish before submitting job B) and can manage quite a lot of different cluster scheduling systems. You can use it from the very beginning of your project since it is able to resume a previous run: So if you first implement job A and then (while job A) is running write job B you can simply update your nextflow script. If you then resume the run, nextflow will remember that it already ran job A and start with job B straight away. This is also great if you later need to change a middle part of your analysis - nextflow will basically only rerun what is needed downstream from the the point that you changed. This is super convenient. To run a nextflow pipeline I usually prepare two scripts: analysis.nf: this is the actual pipeline nextflow.config: this is the configuration script (important on the cluster, for small local test not so much) 7.1.1 The pipeline script The analysis.nf is where you put together you analysis. For me, this means that it is basically a series interconnected bash snippets. The nextflow script is build from three types of building blocks: Processes: the bash snippets that contain the code I would otherwise type into the terminal Channels: streams of files you put through you pipeline, or streams of parameter values you want vary within your analysis Operators: connect the different Processes and Channels using logic (linear, crosses), or modify a channel along the way (filtering, merging, sorting) Processes A nextflow process looks like this: process gather_depth { label &apos;L_loc_test_script&apos; publishDir &quot;export_folder&quot;, mode: &apos;copy&apos; input: set val( input_val ), file( input_file ) from input_channel output: file( &quot;${input_val}.txt&quot; ) into output_channel script: &quot;&quot;&quot; sed &apos;s/X-TAG-X/${input_val}/g&apos; ${input_file} &gt; ${input_val}.txt cat \\$HOME/signature.txt &gt;&gt; ${input_val}.txt &quot;&quot;&quot; } This breaks down into different compartments: process /* &lt;process_name&gt; */ { /* &lt;config section&gt; */ input: /* &lt;input channel&gt; */ output: /* &lt;output channel&gt; */ script: /* &lt;task&gt; */ &quot;&quot;&quot; # some bash code &quot;&quot;&quot; } In the config section we can assign the process to a label (to use specific config settings defined in nextflow.config). Here we can also specify if we want to collect the output of the script outside of the work folder which is used by nextflow for the execution of the code. You can also activate conda environments or load modules for this process here. The input and output sections are used to connect the individual processes and to manage the flow of files and values from process to process. The script section contains the actual code to run. Here, you write the actual bash code of the pipeline. Careful when using bash variables (eg $WORK) within nextflow. Nextflow itself uses the dollar sign ($) to assign its own variables (eg ${input_val}) in the example scripts. These are evaluated before the scripts is run and replaced by the appropriate file/value. If you want to use bash variables that are supposed to be evaluated when the script is actually run, you need to conserve the dollar sign by escaping the dollar sign (cat \\$HOME/signature.txt instead of cat $HOME/signature.txt). Channels These are the streams of files or values. One channel could be eg. all the sequencing files you need to process, while another one could be all the values of an input parameter of of a model that you wish to test for different input settings. Channels connect the different processes and Nextflow basically follows development of the stating channels during the development through the different steps of your pipeline. You feed a channel into a process in the input section, while the output section emits a new channel. At the start of your pipeline you will need to define some channels like this: Channel.from( &apos;A&apos; &apos;B&apos; &apos;C&apos; ) Channel.from( (&apos;01&apos;..&apos;09&apos;) + (&apos;10&apos;..&apos;19&apos;)) Channel.fromPath( &apos;genotypes.vcf.gz&apos; ) Channel.fromPath( &apos;sequencing_data/*.fq.gz&apos; ) Operators Operators contain the actions/logic that connects different elements of your pipeline. You can use them to modify your channels by exposing them to filtering, transformations, splitting, merging. We already saw two operators in the example above (.from() &amp; .fromPath()). So you can see that the have form of dot-name() (or dot-name{} ). Operators (besides .from(), .fromPath() $ .fromFilePairs()) I frequently use are: .splitCsv(): read csv (combined variables) .set{}: channel name (single copy) .into{}: channel name (multiple copies) .map{}: transform channel content .collect(): collapse the channel .combine(): cross of two channels .join(): merge two channels (sorted) .groupTuple(): collapse the channel (sorted) 7.2 A quick example I want to create a quick example of the power of nextflow. Therefore lets assume we have to do a analysis where we have a set of populations that need to be compared pairwise (eg. calculate pairwise FST along the genome). After this you want to do a running average using different window sizes. I will use dummy commands here to show the workings of nextflow rather than doing an meaningful analysis: cd ~/root_folder echo &quot;I am the input data&quot; &gt; data_file.txt The beauty of nextflow is that you basically just have to write the script for a single case. (You can check the complete nextflow on the demo_root_folder reopsitory) First we initialize the different Channels within our pipeline script. Channel .fromPath(&quot;data_file.txt&quot;) .set{ data_channel } Channel .from( [[1, &quot;popA&quot;], [2, &quot;popB&quot;], [3, &quot;popC&quot;]] ) .into{ pop_channel1; pop_channel2 } Channel .from( &quot;10kb&quot; &quot;50kb&quot; ) .set{ span_channel } Then, we combine the different channels using operators: data_channel /* start with the raw data */ .combine( pop_channel1 ) /* add pop1 to data */ .combine( pop_channel2 ) /* cross with pop2 */ .filter{ it[1] &lt; it[3] } /* discad the upper triangle of the cross */ .map{ it[0,2,4]} /* select only data &amp; pops (remove indexes) */ .combine( span_channel ) /* cross with sensitivities */ .set{ pairs_channel } /* name output channel */ Finally, we run the actual bash commands: process run_pairewise { publishDir &quot;output/${span}&quot;, mode: &apos;copy&apos; input: set file( data ), val( pop1 ), val( pop2 ), val( span ) from pairs_channel output: file( &quot;step1.${pop1}-${pop2}.${span}.txt&quot; ) into step1_channel script: &quot;&quot;&quot; cat ${data} &gt; step1.${pop1}-${pop2}.${span}.txt # check data content echo &quot;${pop1} vs. ${pop2}&quot; &gt;&gt; step1.${pop1}-${pop2}.${span}.txt # run pairewise &apos;fst&apos; echo &quot;-- ${span} --&quot; &gt;&gt; step1.${pop1}-${pop2}.${span}.txt # running average &quot;&quot;&quot; } That’s all it takes: cd ~/root_folder ls -1 #&gt; analysis.Rproj #&gt; analysis_twisst.nf #&gt; data #&gt; data_file.txt #&gt; docs #&gt; logo.svg #&gt; nextflow.config #&gt; py #&gt; R #&gt; README.md #&gt; sh nextflow run analysis_twisst.nf #&gt; N E X T F L O W ~ version 0.31.1 #&gt; Launching `analysis_twisst.nf` [gloomy_goldstine] - revision: 5e861dc34e #&gt; [warm up] executor &gt; local #&gt; [d7/fccc1d] Submitted process &gt; run_pairewise (1) #&gt; [5d/285ab0] Submitted process &gt; run_pairewise (4) #&gt; [90/f10c05] Submitted process &gt; run_pairewise (2) #&gt; [5c/371178] Submitted process &gt; run_pairewise (3) #&gt; [79/c63706] Submitted process &gt; run_pairewise (5) #&gt; [4a/ccae53] Submitted process &gt; run_pairewise (6) ls -1a #&gt; . #&gt; .. #&gt; analysis.nf #&gt; analysis.Rproj #&gt; data #&gt; data_file.txt #&gt; docs #&gt; .git #&gt; .gitignore #&gt; logo.svg #&gt; .nextflow &lt;- folder created by nextflow #&gt; nextflow.config #&gt; .nextflow.log &lt;- logfile created by nextflow #&gt; output &lt;- folder created by nextflow #&gt; py #&gt; R #&gt; README.md #&gt; sh #&gt; work &lt;- folder created by nextflow tree output/ #&gt; output/ #&gt; ├── 10kb #&gt; │   ├── step1.popA-popB.10kb.txt #&gt; │   ├── step1.popA-popC.10kb.txt #&gt; │   └── step1.popB-popC.10kb.txt #&gt; └── 50kb #&gt; ├── step1.popA-popB.50kb.txt #&gt; ├── step1.popA-popC.50kb.txt #&gt; └── step1.popB-popC.50kb.txt #&gt; #&gt; 2 directories, 6 files cat output/10kb/step1.popA-popC.10kb.txt #&gt; I am the input data #&gt; popA vs. popC #&gt; -- 10kb -- We can see that the three pair wise comparison were completed for both averaging sensitivities, since all the expected output files show up in the output folder. But nextflow created a few more files and folders: .nextflow: the folder where nextflow does its housekeeping .nextflow.log: the log of the nextflow run output: the output folder (we specifically asked for this one within the process) work: the folder where nextflow executes the scripts. The weird prefixes (eg. [d7/fccc1d]) in the terminal output refer to sub directories of work Of course I don not want git to remember the whole messy nextflow inner workings (the scripts is enough). So, I quickly add those files to the .gitignore (make sure to use double &gt;&gt;!). echo &quot;.nextflow*&quot; &gt;&gt; .gitignore echo &quot;work&quot; &gt;&gt; .gitignore echo &quot;output&quot; &gt;&gt; .gitignore Now, we made some changes - so lets update the repository (and github). First, only the .gitignore. git status #&gt; On branch master #&gt; Your branch is up-to-date with &#39;origin/master&#39;. #&gt; Changes not staged for commit: #&gt; (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) #&gt; (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) #&gt; #&gt; modified: .gitignore #&gt; modified: analysis.nf #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; data_file.txt #&gt; #&gt; no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) git add .gitignore git commit -m &quot;update .gitignore&quot; #&gt; [master 5de89de] update .gitignore #&gt; 1 file changed, 3 insertions(+) Then we add the rest: git status #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 1 commit. #&gt; Changes not staged for commit: #&gt; (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) #&gt; (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) #&gt; #&gt; modified: analysis.nf #&gt; #&gt; Untracked files: #&gt; (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) #&gt; #&gt; data_file.txt #&gt; #&gt; nothing added to commit but untracked files present (use &quot;git add&quot; to track) git add . git commit -m &#39;pipeline update&#39; #&gt; [master 557d3d7] pipeline update #&gt; 2 files changed, 41 insertions(+) #&gt; create mode 100644 data_file.txt git status #&gt; On branch master #&gt; Your branch is ahead of &#39;origin/master&#39; by 2 commits. #&gt; (use &quot;git push&quot; to publish your local commits) #&gt; nothing to commit, working directory clean git push origin master #&gt; Counting objects: 13, done. #&gt; Delta compression using up to 4 threads. #&gt; Compressing objects: 100% (10/10), done. #&gt; Writing objects: 100% (13/13), 1.74 KiB | 0 bytes/s, done. #&gt; Total 13 (delta 4), reused 0 (delta 0) #&gt; remote: Resolving deltas: 100% (4/4), completed with 1 local object. #&gt; To https://github.com/k-hench/demo_root_folder.git #&gt; d8b99be..557d3d7 master -&gt; master Now, in the real world, I would write the nextflow script locally. Then I would commit my changes to github and pull them onto the cluster from there. Finally I would run nextflow on the cluster (nextflow run analysis.nf). 7.3 Managing the workflow on the cluster The nice thing about running nextflow on the cluster is, that it works pretty much the same as locally - so you don’t have to deal with submitting jobs because nextflow will take care of this. The tricky part about this is that you will need to keep nextflow running for the whole time - yet we still want to be able to log out from the cluster and shut down the laptop at the end of the day (and not keep an open ssh session for 10 days until the analysis has finished). For this we can use screen. Basically we create a virtual session that will continue to run even if we log out from the cluster (even though we thereby close the session that created it in the first place). This is ideal for our nextflow needs: It enables us to to log on to the cluster and create a screen session start the nextflow pipeline from within screen detatch from the screen session by pressing &lt;&lt;ctrl&gt;&lt;a&gt;&gt; &lt;&lt;ctrl&gt;&lt;d&gt;&gt; log out from the cluster shut down our laptop (two days later) log on to the cluster reattach to the old screen session &lt;screen -DR&gt; or screen -d -r &lt;session-id&gt; check if nextflow is still running (and kill it since we by now found a typo in the pipeline…) screen -S test while sleep 5; do date;done # &lt;&lt;ctrl&gt;&lt;a&gt;&gt; &lt;&lt;ctrl&gt;&lt;d&gt;&gt; (detach) #&gt; [detached from 14079.test] screen -ls #&gt; There are screens on: #&gt; 14079.test (13.03.2019 13:52:30) (Detached) #&gt; 6972.Tel (13.03.2019 10:53:32) (Detached) #&gt; 2 Sockets in /var/run/screen/S-khench. screen -d -r 14079 #&gt; Mi 13. Mär 13:52:52 CET 2019 #&gt; Mi 13. Mär 13:52:57 CET 2019 #&gt; Mi 13. Mär 13:53:02 CET 2019 #&gt; Mi 13. Mär 13:53:07 CET 2019 #&gt; Mi 13. Mär 13:53:12 CET 2019 #&gt; Mi 13. Mär 13:53:17 CET 2019 #&gt; Mi 13. Mär 13:53:22 CET 2019 #&gt; Mi 13. Mär 13:53:27 CET 2019 # &lt;&lt;ctrl&gt;&lt;c&gt;&gt; (kill while loop) #&gt; ^C exit #&gt; [screen is terminating] screen -ls #&gt; There is a screen on: #&gt; 6972.Tel (13.03.2019 10:53:32) (Detached) #&gt; 1 Socket in /var/run/screen/S-khench. There is one more trick when using screen with the cluster: My metaphor with the hotel was not exactly accurate (surprise…). That is because there are actually three log in nodes instead of a single one and with the standard log in, the log in node is chosen at random (I think). Now if you start a screen session on on log in node and later log on to a different one, you will not find you screen session back. Luckily, you also choose a specific log in node by using eg nesh-fe2 instead of nesh-fe2. To make things a little easier, I added these two lines to my ~/.bashrc file: alias toNEC=&quot;ssh smomw000@nesh-fe1.rz.uni-kiel.de&quot; alias mountNEC=&quot;sshfs smomw000@nesh-fe.rz.uni-kiel.de:/sfs/fs2/work-geomar7/smomw000 ~/mnt&quot; Of course you will need to adjust the mounting path to match your $WORK directory on the cluster. Now, all I need to type to mount/log on to the cluster is toNEC or mountNEC and I will always log onto the same log in node. 7.4 The config script The nextflow.config sets up some global setting for your nextflow run. To use the NEC computer cluster from Uni Kiel, I add this section to the config script: env.BASE_DIR = &quot;$WORK/project_dir&quot; process { executor = &#39;nqsii&#39; queue = &#39;clmedium&#39; cpus = 1 memory = 4.GB time = { 1.h * task.attempt } errorStrategy = { task.exitStatus == Integer.MAX_VALUE ? &#39;retry&#39; : &#39;finish&#39; } maxRetries = 1 maxErrors = &#39;-1&#39; withLabel: &quot;L_loc.*&quot; { executor=&#39;local&#39; } withLabel: &quot;L_20g15m_.*&quot; { queue = &#39;clexpress&#39; memory = 20.GB time = { 15.m * task.attempt } } withLabel: &quot;L_---_.*&quot; { clusterOptions = &#39;---&#39; queue = &#39;---&#39; memory = ---.GB cpus = --- time = { ---.h * task.attempt } } } trace { enabled = true } report { enabled = true } So, I set at few defaults for the jobs, and then stat defining different the types of processes that I reference within the analysis.nf script. Please refer to nextflow documentation for the different options set within the process section. Apart from this, I also provide the path to the project folder and turn the trace and the report on by default. 7.5 Quick recap So, in a nutshell my workflow with nextflow is as follows Initialize the repository (including the analysis.nf) locally Push to github Pull to cluster Start nextflow (cluster) develop further steps &amp; correct bugs of the analysis.nf (locally) update github &amp; cluster resume nextflow to run the new parts of the pipeline (cluster) rinse and repeat until the analysis is complete To resume a nextflow run: cd $WORK/root_folder nextflow run analysis.nf -c nextflow.config -resume By the way - you can also toggle a nice graphical summary of your pipeline using nextflow run analysis.nf -with-dag analysis.png -c nextflow.config -resume: "],
["common-file-types-and-other-software.html", "8 Common file types and other software 8.1 Docker 8.2 File types 8.3 Software", " 8 Common file types and other software 8.1 Docker One thing I have not covered here but which puts the whole reproducible issue to yet another level would be the use of docker. This would allow you to not just share your data &amp; scripts, but also the exact software versions that you used. Frankly, I did not include this here because I do not use it myself (didn’t get the chance to learn how to use it yet). 8.2 File types Just a very superficial register of the most frequently used file types that you should probably know. All of these are actually plain text files and you can open them in regular text editor (don’t do this if the files are large….). The difference is just in the formatting conventions and in the expected content. txt : any type of text md : text with minimal layout code csv : comma separated values (example with thee columns) A,important text,3 tsv : tab separated values A\\timportant text\\t3 8.2 Genetic data fa : aka. fasta - plain genetic sequences, includes an header line per sequence starting with &gt;. (On the right is an example with two sequences - seq1 &amp; seq2) &gt;seq1 ATGCGT GCATGG &gt;seq2 ATGTAA fq : aka. fastq - sequencing data with quality score. (On the right is an example of a sequence - seq1) @seq1 ATGCGTGCATGG # *55CCF&gt;&gt;&#39;&#39;)) sam/bam : sequence alignment format. Genetic sequences mapped to a reference, header lines start with @. (sam is human readable, bam is binary - only for computers) vcf/bcf : variant call format. Genotypes + metadata in table form, header lines start with ##. (vcf is human readable, bcf is binary - only for computers) bed : Browser Extensible Data. Ranges on a reference genome. Includes at least three tab separated columns (chromosome, start, end, example with three ranges). LG02 0 400 LG02 1500 3000 LG15 555 1200 gff : general feature format. It describes exons, genes and other features of DNA. The structure is similar to a bed file with additional columns. 8.2 Code sh : code written in bash R : code written in R py : code written in python pl : code written in pearl nf : code written in nextflow 8.3 Software Below I list what I think are the must haves for any bioinformatic tool shed: fastqc : tool for quality checking of sequencing data (first step of any project using new sequencing data) multiqc : summarize the fastqc reports for all your samples in a single report samtools : tool set for working with sam files vcftools : tool set for working with vcf files (reformatting &amp; population statistics) vcflib : convenience scripts for working with vcf files bedtools : tool set for working with bed files "]
]
